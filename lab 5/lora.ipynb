{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f396f201-d921-4fe4-95b5-ff7237a0fcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: transformers in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (5.0.0)\n",
      "Requirement already satisfied: datasets in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: anyio in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: certifi in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from typer-slim->transformers) (8.1.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: peft in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (5.0.0)\n",
      "Requirement already satisfied: tqdm in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: safetensors in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from peft) (1.4.0)\n",
      "Requirement already satisfied: filelock in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2024.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: anyio in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (4.9.0)\n",
      "Requirement already satisfied: certifi in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (1.0.9)\n",
      "Requirement already satisfied: idna in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers->peft) (0.22.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from typer-slim->huggingface_hub>=0.25.0->peft) (8.1.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: evaluate in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: transformers[torch] in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (5.0.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (1.4.0)\n",
      "Requirement already satisfied: packaging in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers[torch]) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: torch>=2.2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers[torch]) (2.8.0)\n",
      "Requirement already satisfied: accelerate>=1.1.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from transformers[torch]) (1.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: anyio in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: certifi in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: psutil in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from accelerate>=1.1.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (23.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: setuptools in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages (from typer-slim->transformers[torch]) (8.1.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers datasets\n",
    "!pip install peft\n",
    "!pip install evaluate scikit-learn transformers[torch]\n",
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50179772-8165-4a2e-9b51-c86942fa2721",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1c7119-6624-4604-b4b0-f15c905390d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4cf9196697491b9d4c9fbcb2b08ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31c6cef34214a929862232038b1c442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0f5ccf732844ebb90dc87bf21c5ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c26c67b123647f6a5a3025c8db5ae67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(â€¦):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a1606200264b0c88c6bba654173731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d52dd8fe04413aac6f745d349757b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260bf7b13a704f9089c500be7dd2603a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d5236a720242c0af60e50c47f5b72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55debe91496643f3a641a6d359883467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dd5a51163d4e9a917f79ac14fbe3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387d913bf0d644b4aad4ca917998ec90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5351cf3a8414b929f253a61348db8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92ebba40cd945b082ae6c625948d25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90d083b086c41709d54d6ecf569b386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"stanfordnlp/imdb\")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = imdb.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6714ed0a-053f-41d5-b340-353a6d2b3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tokenized_datasets['train'].shuffle(42).select(range(5000))\n",
    "shuffled_set =  tokenized_datasets['test'].shuffle(42)\n",
    "validation_set  = shuffled_set.select(range(1000))\n",
    "test_set = shuffled_set.select(range(1000, 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624c1dc-cbca-4dd0-959f-e482182d94aa",
   "metadata": {},
   "source": [
    "## First method : Using the transformers library and pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25693f50-39a8-4d5f-97bb-c8f1a5ccefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0176c44",
   "metadata": {},
   "source": [
    "## Fine-tuning the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf32374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed65d8eddd774ffa9447c380893b203e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c82b0fa943547a29a60b203cbbe7c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af2cd7fecfd4592902cd6b950d76b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb48a0e118624c9bac6858a62260e6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b45bd94c0343e3b6ae4be9f40986b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: bert-base-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f0272",
   "metadata": {},
   "source": [
    "## LoRA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12fe280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings.word_embeddings.weight',\n",
       " 'embeddings.position_embeddings.weight',\n",
       " 'embeddings.token_type_embeddings.weight',\n",
       " 'embeddings.LayerNorm.weight',\n",
       " 'embeddings.LayerNorm.bias',\n",
       " 'encoder.layer.0.attention.self.query.weight',\n",
       " 'encoder.layer.0.attention.self.query.bias',\n",
       " 'encoder.layer.0.attention.self.key.weight',\n",
       " 'encoder.layer.0.attention.self.key.bias',\n",
       " 'encoder.layer.0.attention.self.value.weight',\n",
       " 'encoder.layer.0.attention.self.value.bias',\n",
       " 'encoder.layer.0.attention.output.dense.weight',\n",
       " 'encoder.layer.0.attention.output.dense.bias',\n",
       " 'encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.0.intermediate.dense.weight',\n",
       " 'encoder.layer.0.intermediate.dense.bias',\n",
       " 'encoder.layer.0.output.dense.weight',\n",
       " 'encoder.layer.0.output.dense.bias',\n",
       " 'encoder.layer.0.output.LayerNorm.weight',\n",
       " 'encoder.layer.0.output.LayerNorm.bias',\n",
       " 'encoder.layer.1.attention.self.query.weight',\n",
       " 'encoder.layer.1.attention.self.query.bias',\n",
       " 'encoder.layer.1.attention.self.key.weight',\n",
       " 'encoder.layer.1.attention.self.key.bias',\n",
       " 'encoder.layer.1.attention.self.value.weight',\n",
       " 'encoder.layer.1.attention.self.value.bias',\n",
       " 'encoder.layer.1.attention.output.dense.weight',\n",
       " 'encoder.layer.1.attention.output.dense.bias',\n",
       " 'encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.1.intermediate.dense.weight',\n",
       " 'encoder.layer.1.intermediate.dense.bias',\n",
       " 'encoder.layer.1.output.dense.weight',\n",
       " 'encoder.layer.1.output.dense.bias',\n",
       " 'encoder.layer.1.output.LayerNorm.weight',\n",
       " 'encoder.layer.1.output.LayerNorm.bias',\n",
       " 'encoder.layer.2.attention.self.query.weight',\n",
       " 'encoder.layer.2.attention.self.query.bias',\n",
       " 'encoder.layer.2.attention.self.key.weight',\n",
       " 'encoder.layer.2.attention.self.key.bias',\n",
       " 'encoder.layer.2.attention.self.value.weight',\n",
       " 'encoder.layer.2.attention.self.value.bias',\n",
       " 'encoder.layer.2.attention.output.dense.weight',\n",
       " 'encoder.layer.2.attention.output.dense.bias',\n",
       " 'encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.2.intermediate.dense.weight',\n",
       " 'encoder.layer.2.intermediate.dense.bias',\n",
       " 'encoder.layer.2.output.dense.weight',\n",
       " 'encoder.layer.2.output.dense.bias',\n",
       " 'encoder.layer.2.output.LayerNorm.weight',\n",
       " 'encoder.layer.2.output.LayerNorm.bias',\n",
       " 'encoder.layer.3.attention.self.query.weight',\n",
       " 'encoder.layer.3.attention.self.query.bias',\n",
       " 'encoder.layer.3.attention.self.key.weight',\n",
       " 'encoder.layer.3.attention.self.key.bias',\n",
       " 'encoder.layer.3.attention.self.value.weight',\n",
       " 'encoder.layer.3.attention.self.value.bias',\n",
       " 'encoder.layer.3.attention.output.dense.weight',\n",
       " 'encoder.layer.3.attention.output.dense.bias',\n",
       " 'encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.3.intermediate.dense.weight',\n",
       " 'encoder.layer.3.intermediate.dense.bias',\n",
       " 'encoder.layer.3.output.dense.weight',\n",
       " 'encoder.layer.3.output.dense.bias',\n",
       " 'encoder.layer.3.output.LayerNorm.weight',\n",
       " 'encoder.layer.3.output.LayerNorm.bias',\n",
       " 'encoder.layer.4.attention.self.query.weight',\n",
       " 'encoder.layer.4.attention.self.query.bias',\n",
       " 'encoder.layer.4.attention.self.key.weight',\n",
       " 'encoder.layer.4.attention.self.key.bias',\n",
       " 'encoder.layer.4.attention.self.value.weight',\n",
       " 'encoder.layer.4.attention.self.value.bias',\n",
       " 'encoder.layer.4.attention.output.dense.weight',\n",
       " 'encoder.layer.4.attention.output.dense.bias',\n",
       " 'encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.4.intermediate.dense.weight',\n",
       " 'encoder.layer.4.intermediate.dense.bias',\n",
       " 'encoder.layer.4.output.dense.weight',\n",
       " 'encoder.layer.4.output.dense.bias',\n",
       " 'encoder.layer.4.output.LayerNorm.weight',\n",
       " 'encoder.layer.4.output.LayerNorm.bias',\n",
       " 'encoder.layer.5.attention.self.query.weight',\n",
       " 'encoder.layer.5.attention.self.query.bias',\n",
       " 'encoder.layer.5.attention.self.key.weight',\n",
       " 'encoder.layer.5.attention.self.key.bias',\n",
       " 'encoder.layer.5.attention.self.value.weight',\n",
       " 'encoder.layer.5.attention.self.value.bias',\n",
       " 'encoder.layer.5.attention.output.dense.weight',\n",
       " 'encoder.layer.5.attention.output.dense.bias',\n",
       " 'encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.5.intermediate.dense.weight',\n",
       " 'encoder.layer.5.intermediate.dense.bias',\n",
       " 'encoder.layer.5.output.dense.weight',\n",
       " 'encoder.layer.5.output.dense.bias',\n",
       " 'encoder.layer.5.output.LayerNorm.weight',\n",
       " 'encoder.layer.5.output.LayerNorm.bias',\n",
       " 'encoder.layer.6.attention.self.query.weight',\n",
       " 'encoder.layer.6.attention.self.query.bias',\n",
       " 'encoder.layer.6.attention.self.key.weight',\n",
       " 'encoder.layer.6.attention.self.key.bias',\n",
       " 'encoder.layer.6.attention.self.value.weight',\n",
       " 'encoder.layer.6.attention.self.value.bias',\n",
       " 'encoder.layer.6.attention.output.dense.weight',\n",
       " 'encoder.layer.6.attention.output.dense.bias',\n",
       " 'encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.6.intermediate.dense.weight',\n",
       " 'encoder.layer.6.intermediate.dense.bias',\n",
       " 'encoder.layer.6.output.dense.weight',\n",
       " 'encoder.layer.6.output.dense.bias',\n",
       " 'encoder.layer.6.output.LayerNorm.weight',\n",
       " 'encoder.layer.6.output.LayerNorm.bias',\n",
       " 'encoder.layer.7.attention.self.query.weight',\n",
       " 'encoder.layer.7.attention.self.query.bias',\n",
       " 'encoder.layer.7.attention.self.key.weight',\n",
       " 'encoder.layer.7.attention.self.key.bias',\n",
       " 'encoder.layer.7.attention.self.value.weight',\n",
       " 'encoder.layer.7.attention.self.value.bias',\n",
       " 'encoder.layer.7.attention.output.dense.weight',\n",
       " 'encoder.layer.7.attention.output.dense.bias',\n",
       " 'encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.7.intermediate.dense.weight',\n",
       " 'encoder.layer.7.intermediate.dense.bias',\n",
       " 'encoder.layer.7.output.dense.weight',\n",
       " 'encoder.layer.7.output.dense.bias',\n",
       " 'encoder.layer.7.output.LayerNorm.weight',\n",
       " 'encoder.layer.7.output.LayerNorm.bias',\n",
       " 'encoder.layer.8.attention.self.query.weight',\n",
       " 'encoder.layer.8.attention.self.query.bias',\n",
       " 'encoder.layer.8.attention.self.key.weight',\n",
       " 'encoder.layer.8.attention.self.key.bias',\n",
       " 'encoder.layer.8.attention.self.value.weight',\n",
       " 'encoder.layer.8.attention.self.value.bias',\n",
       " 'encoder.layer.8.attention.output.dense.weight',\n",
       " 'encoder.layer.8.attention.output.dense.bias',\n",
       " 'encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.8.intermediate.dense.weight',\n",
       " 'encoder.layer.8.intermediate.dense.bias',\n",
       " 'encoder.layer.8.output.dense.weight',\n",
       " 'encoder.layer.8.output.dense.bias',\n",
       " 'encoder.layer.8.output.LayerNorm.weight',\n",
       " 'encoder.layer.8.output.LayerNorm.bias',\n",
       " 'encoder.layer.9.attention.self.query.weight',\n",
       " 'encoder.layer.9.attention.self.query.bias',\n",
       " 'encoder.layer.9.attention.self.key.weight',\n",
       " 'encoder.layer.9.attention.self.key.bias',\n",
       " 'encoder.layer.9.attention.self.value.weight',\n",
       " 'encoder.layer.9.attention.self.value.bias',\n",
       " 'encoder.layer.9.attention.output.dense.weight',\n",
       " 'encoder.layer.9.attention.output.dense.bias',\n",
       " 'encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.9.intermediate.dense.weight',\n",
       " 'encoder.layer.9.intermediate.dense.bias',\n",
       " 'encoder.layer.9.output.dense.weight',\n",
       " 'encoder.layer.9.output.dense.bias',\n",
       " 'encoder.layer.9.output.LayerNorm.weight',\n",
       " 'encoder.layer.9.output.LayerNorm.bias',\n",
       " 'encoder.layer.10.attention.self.query.weight',\n",
       " 'encoder.layer.10.attention.self.query.bias',\n",
       " 'encoder.layer.10.attention.self.key.weight',\n",
       " 'encoder.layer.10.attention.self.key.bias',\n",
       " 'encoder.layer.10.attention.self.value.weight',\n",
       " 'encoder.layer.10.attention.self.value.bias',\n",
       " 'encoder.layer.10.attention.output.dense.weight',\n",
       " 'encoder.layer.10.attention.output.dense.bias',\n",
       " 'encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.10.intermediate.dense.weight',\n",
       " 'encoder.layer.10.intermediate.dense.bias',\n",
       " 'encoder.layer.10.output.dense.weight',\n",
       " 'encoder.layer.10.output.dense.bias',\n",
       " 'encoder.layer.10.output.LayerNorm.weight',\n",
       " 'encoder.layer.10.output.LayerNorm.bias',\n",
       " 'encoder.layer.11.attention.self.query.weight',\n",
       " 'encoder.layer.11.attention.self.query.bias',\n",
       " 'encoder.layer.11.attention.self.key.weight',\n",
       " 'encoder.layer.11.attention.self.key.bias',\n",
       " 'encoder.layer.11.attention.self.value.weight',\n",
       " 'encoder.layer.11.attention.self.value.bias',\n",
       " 'encoder.layer.11.attention.output.dense.weight',\n",
       " 'encoder.layer.11.attention.output.dense.bias',\n",
       " 'encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.11.intermediate.dense.weight',\n",
       " 'encoder.layer.11.intermediate.dense.bias',\n",
       " 'encoder.layer.11.output.dense.weight',\n",
       " 'encoder.layer.11.output.dense.bias',\n",
       " 'encoder.layer.11.output.LayerNorm.weight',\n",
       " 'encoder.layer.11.output.LayerNorm.bias',\n",
       " 'pooler.dense.weight',\n",
       " 'pooler.dense.bias']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k,v  in model.bert.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "759b1ef3-25e9-44aa-82e0-af70ce76abfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the model \n",
      "\n",
      " BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"What is the model \\n\\n {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82be16-830b-474b-b5f6-a98df468391f",
   "metadata": {},
   "source": [
    "### Create the LoRA module\n",
    "\n",
    "We will consider in this module the original linear, and the down and up projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ba190fe-401c-411c-9958-1e74d51bcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, rank: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.lora_down = nn.Linear(in_dim, rank, bias=False)\n",
    "        self.lora_up = nn.Linear(rank, out_dim, bias=False)\n",
    "\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x) + self.lora_up(self.lora_down(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32301774-a17f-44dd-a7b3-e7aa8d15e655",
   "metadata": {},
   "source": [
    "### Create a function replacing linear by lora module\n",
    "From the original Linear module return a LoRA module\n",
    "* The linear of the LoRA linear must be initialised with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cbf7015-fdd5-41ea-b865-5d2021b970d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_to_lora(linear, rank=8):\n",
    "    has_bias = linear.bias is not None\n",
    "    out_dim, in_dim = linear.weight.shape\n",
    "\n",
    "    lora = LoRALinear(in_dim, out_dim, rank=rank).to(linear.weight.device, linear.weight.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lora.linear.weight.copy_(linear.weight)\n",
    "        if has_bias:\n",
    "            lora.linear.bias.copy_(linear.bias)\n",
    "\n",
    "    return lora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e3fac-0662-4f41-937f-d9736800103c",
   "metadata": {},
   "source": [
    "We now replace the target linear by the LoRALinear described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d0fb24-cdaa-4cbd-ac75-a00a12c9a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "lora_model = copy.deepcopy(model)\n",
    "lora_parameters = []\n",
    "\n",
    "for layer in lora_model.bert.encoder.layer:\n",
    "    layer.attention.self.query = linear_to_lora(layer.attention.self.query)\n",
    "    layer.attention.self.key = linear_to_lora(layer.attention.self.key)\n",
    "    layer.attention.self.value = linear_to_lora(layer.attention.self.value)\n",
    "    layer.attention.output.dense = linear_to_lora(layer.attention.output.dense)\n",
    "    layer.intermediate.dense = linear_to_lora(layer.intermediate.dense)\n",
    "    layer.output.dense = linear_to_lora(layer.output.dense)\n",
    "\n",
    "    lora_parameters += list(layer.attention.self.query.lora_down.parameters())\n",
    "    lora_parameters += list(layer.attention.self.query.lora_up.parameters())\n",
    "    lora_parameters += list(layer.attention.self.key.lora_down.parameters())\n",
    "    lora_parameters += list(layer.attention.self.key.lora_up.parameters())\n",
    "    lora_parameters += list(layer.attention.self.value.lora_down.parameters())\n",
    "    lora_parameters += list(layer.attention.self.value.lora_up.parameters())\n",
    "    lora_parameters += list(layer.attention.output.dense.lora_down.parameters())\n",
    "    lora_parameters += list(layer.attention.output.dense.lora_up.parameters())\n",
    "    lora_parameters += list(layer.intermediate.dense.lora_down.parameters())\n",
    "    lora_parameters += list(layer.intermediate.dense.lora_up.parameters())\n",
    "    lora_parameters += list(layer.output.dense.lora_down.parameters())\n",
    "    lora_parameters += list(layer.output.dense.lora_up.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8484560f-2d4f-4e77-a381-f3b6dbd581d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the model modified with LoRA: \n",
      "\n",
      " BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): LoRALinear(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_down): Linear(in_features=768, out_features=8, bias=False)\n",
      "                (lora_up): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (key): LoRALinear(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_down): Linear(in_features=768, out_features=8, bias=False)\n",
      "                (lora_up): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (value): LoRALinear(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_down): Linear(in_features=768, out_features=8, bias=False)\n",
      "                (lora_up): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): LoRALinear(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_down): Linear(in_features=768, out_features=8, bias=False)\n",
      "                (lora_up): Linear(in_features=8, out_features=768, bias=False)\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): LoRALinear(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora_down): Linear(in_features=768, out_features=8, bias=False)\n",
      "              (lora_up): Linear(in_features=8, out_features=3072, bias=False)\n",
      "            )\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): LoRALinear(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora_down): Linear(in_features=3072, out_features=8, bias=False)\n",
      "              (lora_up): Linear(in_features=8, out_features=768, bias=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"What is the model modified with LoRA: \\n\\n {lora_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad8e4a-2d59-47c9-87dc-3b26836dada0",
   "metadata": {},
   "source": [
    "### Defining module requiring grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00bed4e1-e5ef-4cfe-83a8-dc915bc3070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.linear.weight\n",
      "encoder.layer.0.attention.self.query.linear.bias\n",
      "encoder.layer.0.attention.self.query.lora_down.weight\n",
      "encoder.layer.0.attention.self.query.lora_up.weight\n",
      "encoder.layer.0.attention.self.key.linear.weight\n",
      "encoder.layer.0.attention.self.key.linear.bias\n",
      "encoder.layer.0.attention.self.key.lora_down.weight\n",
      "encoder.layer.0.attention.self.key.lora_up.weight\n",
      "encoder.layer.0.attention.self.value.linear.weight\n",
      "encoder.layer.0.attention.self.value.linear.bias\n",
      "encoder.layer.0.attention.self.value.lora_down.weight\n",
      "encoder.layer.0.attention.self.value.lora_up.weight\n",
      "encoder.layer.0.attention.output.dense.linear.weight\n",
      "encoder.layer.0.attention.output.dense.linear.bias\n",
      "encoder.layer.0.attention.output.dense.lora_down.weight\n",
      "encoder.layer.0.attention.output.dense.lora_up.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.linear.weight\n",
      "encoder.layer.0.intermediate.dense.linear.bias\n",
      "encoder.layer.0.intermediate.dense.lora_down.weight\n",
      "encoder.layer.0.intermediate.dense.lora_up.weight\n",
      "encoder.layer.0.output.dense.linear.weight\n",
      "encoder.layer.0.output.dense.linear.bias\n",
      "encoder.layer.0.output.dense.lora_down.weight\n",
      "encoder.layer.0.output.dense.lora_up.weight\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.linear.weight\n",
      "encoder.layer.1.attention.self.query.linear.bias\n",
      "encoder.layer.1.attention.self.query.lora_down.weight\n",
      "encoder.layer.1.attention.self.query.lora_up.weight\n",
      "encoder.layer.1.attention.self.key.linear.weight\n",
      "encoder.layer.1.attention.self.key.linear.bias\n",
      "encoder.layer.1.attention.self.key.lora_down.weight\n",
      "encoder.layer.1.attention.self.key.lora_up.weight\n",
      "encoder.layer.1.attention.self.value.linear.weight\n",
      "encoder.layer.1.attention.self.value.linear.bias\n",
      "encoder.layer.1.attention.self.value.lora_down.weight\n",
      "encoder.layer.1.attention.self.value.lora_up.weight\n",
      "encoder.layer.1.attention.output.dense.linear.weight\n",
      "encoder.layer.1.attention.output.dense.linear.bias\n",
      "encoder.layer.1.attention.output.dense.lora_down.weight\n",
      "encoder.layer.1.attention.output.dense.lora_up.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.linear.weight\n",
      "encoder.layer.1.intermediate.dense.linear.bias\n",
      "encoder.layer.1.intermediate.dense.lora_down.weight\n",
      "encoder.layer.1.intermediate.dense.lora_up.weight\n",
      "encoder.layer.1.output.dense.linear.weight\n",
      "encoder.layer.1.output.dense.linear.bias\n",
      "encoder.layer.1.output.dense.lora_down.weight\n",
      "encoder.layer.1.output.dense.lora_up.weight\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.linear.weight\n",
      "encoder.layer.2.attention.self.query.linear.bias\n",
      "encoder.layer.2.attention.self.query.lora_down.weight\n",
      "encoder.layer.2.attention.self.query.lora_up.weight\n",
      "encoder.layer.2.attention.self.key.linear.weight\n",
      "encoder.layer.2.attention.self.key.linear.bias\n",
      "encoder.layer.2.attention.self.key.lora_down.weight\n",
      "encoder.layer.2.attention.self.key.lora_up.weight\n",
      "encoder.layer.2.attention.self.value.linear.weight\n",
      "encoder.layer.2.attention.self.value.linear.bias\n",
      "encoder.layer.2.attention.self.value.lora_down.weight\n",
      "encoder.layer.2.attention.self.value.lora_up.weight\n",
      "encoder.layer.2.attention.output.dense.linear.weight\n",
      "encoder.layer.2.attention.output.dense.linear.bias\n",
      "encoder.layer.2.attention.output.dense.lora_down.weight\n",
      "encoder.layer.2.attention.output.dense.lora_up.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.linear.weight\n",
      "encoder.layer.2.intermediate.dense.linear.bias\n",
      "encoder.layer.2.intermediate.dense.lora_down.weight\n",
      "encoder.layer.2.intermediate.dense.lora_up.weight\n",
      "encoder.layer.2.output.dense.linear.weight\n",
      "encoder.layer.2.output.dense.linear.bias\n",
      "encoder.layer.2.output.dense.lora_down.weight\n",
      "encoder.layer.2.output.dense.lora_up.weight\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.linear.weight\n",
      "encoder.layer.3.attention.self.query.linear.bias\n",
      "encoder.layer.3.attention.self.query.lora_down.weight\n",
      "encoder.layer.3.attention.self.query.lora_up.weight\n",
      "encoder.layer.3.attention.self.key.linear.weight\n",
      "encoder.layer.3.attention.self.key.linear.bias\n",
      "encoder.layer.3.attention.self.key.lora_down.weight\n",
      "encoder.layer.3.attention.self.key.lora_up.weight\n",
      "encoder.layer.3.attention.self.value.linear.weight\n",
      "encoder.layer.3.attention.self.value.linear.bias\n",
      "encoder.layer.3.attention.self.value.lora_down.weight\n",
      "encoder.layer.3.attention.self.value.lora_up.weight\n",
      "encoder.layer.3.attention.output.dense.linear.weight\n",
      "encoder.layer.3.attention.output.dense.linear.bias\n",
      "encoder.layer.3.attention.output.dense.lora_down.weight\n",
      "encoder.layer.3.attention.output.dense.lora_up.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.linear.weight\n",
      "encoder.layer.3.intermediate.dense.linear.bias\n",
      "encoder.layer.3.intermediate.dense.lora_down.weight\n",
      "encoder.layer.3.intermediate.dense.lora_up.weight\n",
      "encoder.layer.3.output.dense.linear.weight\n",
      "encoder.layer.3.output.dense.linear.bias\n",
      "encoder.layer.3.output.dense.lora_down.weight\n",
      "encoder.layer.3.output.dense.lora_up.weight\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.linear.weight\n",
      "encoder.layer.4.attention.self.query.linear.bias\n",
      "encoder.layer.4.attention.self.query.lora_down.weight\n",
      "encoder.layer.4.attention.self.query.lora_up.weight\n",
      "encoder.layer.4.attention.self.key.linear.weight\n",
      "encoder.layer.4.attention.self.key.linear.bias\n",
      "encoder.layer.4.attention.self.key.lora_down.weight\n",
      "encoder.layer.4.attention.self.key.lora_up.weight\n",
      "encoder.layer.4.attention.self.value.linear.weight\n",
      "encoder.layer.4.attention.self.value.linear.bias\n",
      "encoder.layer.4.attention.self.value.lora_down.weight\n",
      "encoder.layer.4.attention.self.value.lora_up.weight\n",
      "encoder.layer.4.attention.output.dense.linear.weight\n",
      "encoder.layer.4.attention.output.dense.linear.bias\n",
      "encoder.layer.4.attention.output.dense.lora_down.weight\n",
      "encoder.layer.4.attention.output.dense.lora_up.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.linear.weight\n",
      "encoder.layer.4.intermediate.dense.linear.bias\n",
      "encoder.layer.4.intermediate.dense.lora_down.weight\n",
      "encoder.layer.4.intermediate.dense.lora_up.weight\n",
      "encoder.layer.4.output.dense.linear.weight\n",
      "encoder.layer.4.output.dense.linear.bias\n",
      "encoder.layer.4.output.dense.lora_down.weight\n",
      "encoder.layer.4.output.dense.lora_up.weight\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.linear.weight\n",
      "encoder.layer.5.attention.self.query.linear.bias\n",
      "encoder.layer.5.attention.self.query.lora_down.weight\n",
      "encoder.layer.5.attention.self.query.lora_up.weight\n",
      "encoder.layer.5.attention.self.key.linear.weight\n",
      "encoder.layer.5.attention.self.key.linear.bias\n",
      "encoder.layer.5.attention.self.key.lora_down.weight\n",
      "encoder.layer.5.attention.self.key.lora_up.weight\n",
      "encoder.layer.5.attention.self.value.linear.weight\n",
      "encoder.layer.5.attention.self.value.linear.bias\n",
      "encoder.layer.5.attention.self.value.lora_down.weight\n",
      "encoder.layer.5.attention.self.value.lora_up.weight\n",
      "encoder.layer.5.attention.output.dense.linear.weight\n",
      "encoder.layer.5.attention.output.dense.linear.bias\n",
      "encoder.layer.5.attention.output.dense.lora_down.weight\n",
      "encoder.layer.5.attention.output.dense.lora_up.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.linear.weight\n",
      "encoder.layer.5.intermediate.dense.linear.bias\n",
      "encoder.layer.5.intermediate.dense.lora_down.weight\n",
      "encoder.layer.5.intermediate.dense.lora_up.weight\n",
      "encoder.layer.5.output.dense.linear.weight\n",
      "encoder.layer.5.output.dense.linear.bias\n",
      "encoder.layer.5.output.dense.lora_down.weight\n",
      "encoder.layer.5.output.dense.lora_up.weight\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.linear.weight\n",
      "encoder.layer.6.attention.self.query.linear.bias\n",
      "encoder.layer.6.attention.self.query.lora_down.weight\n",
      "encoder.layer.6.attention.self.query.lora_up.weight\n",
      "encoder.layer.6.attention.self.key.linear.weight\n",
      "encoder.layer.6.attention.self.key.linear.bias\n",
      "encoder.layer.6.attention.self.key.lora_down.weight\n",
      "encoder.layer.6.attention.self.key.lora_up.weight\n",
      "encoder.layer.6.attention.self.value.linear.weight\n",
      "encoder.layer.6.attention.self.value.linear.bias\n",
      "encoder.layer.6.attention.self.value.lora_down.weight\n",
      "encoder.layer.6.attention.self.value.lora_up.weight\n",
      "encoder.layer.6.attention.output.dense.linear.weight\n",
      "encoder.layer.6.attention.output.dense.linear.bias\n",
      "encoder.layer.6.attention.output.dense.lora_down.weight\n",
      "encoder.layer.6.attention.output.dense.lora_up.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.linear.weight\n",
      "encoder.layer.6.intermediate.dense.linear.bias\n",
      "encoder.layer.6.intermediate.dense.lora_down.weight\n",
      "encoder.layer.6.intermediate.dense.lora_up.weight\n",
      "encoder.layer.6.output.dense.linear.weight\n",
      "encoder.layer.6.output.dense.linear.bias\n",
      "encoder.layer.6.output.dense.lora_down.weight\n",
      "encoder.layer.6.output.dense.lora_up.weight\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.linear.weight\n",
      "encoder.layer.7.attention.self.query.linear.bias\n",
      "encoder.layer.7.attention.self.query.lora_down.weight\n",
      "encoder.layer.7.attention.self.query.lora_up.weight\n",
      "encoder.layer.7.attention.self.key.linear.weight\n",
      "encoder.layer.7.attention.self.key.linear.bias\n",
      "encoder.layer.7.attention.self.key.lora_down.weight\n",
      "encoder.layer.7.attention.self.key.lora_up.weight\n",
      "encoder.layer.7.attention.self.value.linear.weight\n",
      "encoder.layer.7.attention.self.value.linear.bias\n",
      "encoder.layer.7.attention.self.value.lora_down.weight\n",
      "encoder.layer.7.attention.self.value.lora_up.weight\n",
      "encoder.layer.7.attention.output.dense.linear.weight\n",
      "encoder.layer.7.attention.output.dense.linear.bias\n",
      "encoder.layer.7.attention.output.dense.lora_down.weight\n",
      "encoder.layer.7.attention.output.dense.lora_up.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.linear.weight\n",
      "encoder.layer.7.intermediate.dense.linear.bias\n",
      "encoder.layer.7.intermediate.dense.lora_down.weight\n",
      "encoder.layer.7.intermediate.dense.lora_up.weight\n",
      "encoder.layer.7.output.dense.linear.weight\n",
      "encoder.layer.7.output.dense.linear.bias\n",
      "encoder.layer.7.output.dense.lora_down.weight\n",
      "encoder.layer.7.output.dense.lora_up.weight\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.linear.weight\n",
      "encoder.layer.8.attention.self.query.linear.bias\n",
      "encoder.layer.8.attention.self.query.lora_down.weight\n",
      "encoder.layer.8.attention.self.query.lora_up.weight\n",
      "encoder.layer.8.attention.self.key.linear.weight\n",
      "encoder.layer.8.attention.self.key.linear.bias\n",
      "encoder.layer.8.attention.self.key.lora_down.weight\n",
      "encoder.layer.8.attention.self.key.lora_up.weight\n",
      "encoder.layer.8.attention.self.value.linear.weight\n",
      "encoder.layer.8.attention.self.value.linear.bias\n",
      "encoder.layer.8.attention.self.value.lora_down.weight\n",
      "encoder.layer.8.attention.self.value.lora_up.weight\n",
      "encoder.layer.8.attention.output.dense.linear.weight\n",
      "encoder.layer.8.attention.output.dense.linear.bias\n",
      "encoder.layer.8.attention.output.dense.lora_down.weight\n",
      "encoder.layer.8.attention.output.dense.lora_up.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.linear.weight\n",
      "encoder.layer.8.intermediate.dense.linear.bias\n",
      "encoder.layer.8.intermediate.dense.lora_down.weight\n",
      "encoder.layer.8.intermediate.dense.lora_up.weight\n",
      "encoder.layer.8.output.dense.linear.weight\n",
      "encoder.layer.8.output.dense.linear.bias\n",
      "encoder.layer.8.output.dense.lora_down.weight\n",
      "encoder.layer.8.output.dense.lora_up.weight\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.linear.weight\n",
      "encoder.layer.9.attention.self.query.linear.bias\n",
      "encoder.layer.9.attention.self.query.lora_down.weight\n",
      "encoder.layer.9.attention.self.query.lora_up.weight\n",
      "encoder.layer.9.attention.self.key.linear.weight\n",
      "encoder.layer.9.attention.self.key.linear.bias\n",
      "encoder.layer.9.attention.self.key.lora_down.weight\n",
      "encoder.layer.9.attention.self.key.lora_up.weight\n",
      "encoder.layer.9.attention.self.value.linear.weight\n",
      "encoder.layer.9.attention.self.value.linear.bias\n",
      "encoder.layer.9.attention.self.value.lora_down.weight\n",
      "encoder.layer.9.attention.self.value.lora_up.weight\n",
      "encoder.layer.9.attention.output.dense.linear.weight\n",
      "encoder.layer.9.attention.output.dense.linear.bias\n",
      "encoder.layer.9.attention.output.dense.lora_down.weight\n",
      "encoder.layer.9.attention.output.dense.lora_up.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.linear.weight\n",
      "encoder.layer.9.intermediate.dense.linear.bias\n",
      "encoder.layer.9.intermediate.dense.lora_down.weight\n",
      "encoder.layer.9.intermediate.dense.lora_up.weight\n",
      "encoder.layer.9.output.dense.linear.weight\n",
      "encoder.layer.9.output.dense.linear.bias\n",
      "encoder.layer.9.output.dense.lora_down.weight\n",
      "encoder.layer.9.output.dense.lora_up.weight\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.linear.weight\n",
      "encoder.layer.10.attention.self.query.linear.bias\n",
      "encoder.layer.10.attention.self.query.lora_down.weight\n",
      "encoder.layer.10.attention.self.query.lora_up.weight\n",
      "encoder.layer.10.attention.self.key.linear.weight\n",
      "encoder.layer.10.attention.self.key.linear.bias\n",
      "encoder.layer.10.attention.self.key.lora_down.weight\n",
      "encoder.layer.10.attention.self.key.lora_up.weight\n",
      "encoder.layer.10.attention.self.value.linear.weight\n",
      "encoder.layer.10.attention.self.value.linear.bias\n",
      "encoder.layer.10.attention.self.value.lora_down.weight\n",
      "encoder.layer.10.attention.self.value.lora_up.weight\n",
      "encoder.layer.10.attention.output.dense.linear.weight\n",
      "encoder.layer.10.attention.output.dense.linear.bias\n",
      "encoder.layer.10.attention.output.dense.lora_down.weight\n",
      "encoder.layer.10.attention.output.dense.lora_up.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.linear.weight\n",
      "encoder.layer.10.intermediate.dense.linear.bias\n",
      "encoder.layer.10.intermediate.dense.lora_down.weight\n",
      "encoder.layer.10.intermediate.dense.lora_up.weight\n",
      "encoder.layer.10.output.dense.linear.weight\n",
      "encoder.layer.10.output.dense.linear.bias\n",
      "encoder.layer.10.output.dense.lora_down.weight\n",
      "encoder.layer.10.output.dense.lora_up.weight\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.linear.weight\n",
      "encoder.layer.11.attention.self.query.linear.bias\n",
      "encoder.layer.11.attention.self.query.lora_down.weight\n",
      "encoder.layer.11.attention.self.query.lora_up.weight\n",
      "encoder.layer.11.attention.self.key.linear.weight\n",
      "encoder.layer.11.attention.self.key.linear.bias\n",
      "encoder.layer.11.attention.self.key.lora_down.weight\n",
      "encoder.layer.11.attention.self.key.lora_up.weight\n",
      "encoder.layer.11.attention.self.value.linear.weight\n",
      "encoder.layer.11.attention.self.value.linear.bias\n",
      "encoder.layer.11.attention.self.value.lora_down.weight\n",
      "encoder.layer.11.attention.self.value.lora_up.weight\n",
      "encoder.layer.11.attention.output.dense.linear.weight\n",
      "encoder.layer.11.attention.output.dense.linear.bias\n",
      "encoder.layer.11.attention.output.dense.lora_down.weight\n",
      "encoder.layer.11.attention.output.dense.lora_up.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.linear.weight\n",
      "encoder.layer.11.intermediate.dense.linear.bias\n",
      "encoder.layer.11.intermediate.dense.lora_down.weight\n",
      "encoder.layer.11.intermediate.dense.lora_up.weight\n",
      "encoder.layer.11.output.dense.linear.weight\n",
      "encoder.layer.11.output.dense.linear.bias\n",
      "encoder.layer.11.output.dense.lora_down.weight\n",
      "encoder.layer.11.output.dense.lora_up.weight\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v  in lora_model.bert.named_parameters():\n",
    "    print(k)\n",
    "    if ('lora' in k):\n",
    "        v.requires_grad = True\n",
    "    else:\n",
    "        v.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f724d4-b1bb-40c2-8902-ab9ea7fe9bd6",
   "metadata": {},
   "source": [
    "## Using transformers Trainer to fine-tune with LoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6f2f95c-aff2-46a8-abc0-d21bdd663bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d033ff4b53407ba99d490ff50482d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7f4a058-fcfc-448c-b1c5-e945cc7f58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_custom_lora\",\n",
    "                                  eval_strategy=\"steps\",\n",
    "                                  eval_steps= 128,\n",
    "                                  num_train_epochs=2,)\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset= training_set,\n",
    "    eval_dataset=validation_set,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8971eec6-b562-45c4-8c7a-8affe54a4364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [626/626 06:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.260714</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.755160</td>\n",
       "      <td>0.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.729394</td>\n",
       "      <td>0.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.953346</td>\n",
       "      <td>0.633637</td>\n",
       "      <td>0.867000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d244c8e8364d698ae486aa61dc1cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff17bd4441aa4f00a5dc493ba6297a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=626, training_loss=0.8810046138093114, metrics={'train_runtime': 374.1155, 'train_samples_per_second': 26.73, 'train_steps_per_second': 1.673, 'total_flos': 2671879188480000.0, 'train_loss': 0.8810046138093114, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f8f99-26f4-4573-a4d7-7ee87f43f587",
   "metadata": {},
   "source": [
    "## Using PEFT library to fine-tune with LoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94a30566-0231-420d-8709-1be4fb238f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7bbb6b3ba342968bb81c5dfb0e1a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: bert-base-cased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType,  get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels=2\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d361529-f3e8-44b3-8fcd-5640679405f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_custom_lora\",\n",
    "                                  eval_strategy=\"steps\",\n",
    "                                  eval_steps= 128,\n",
    "                                  num_train_epochs=2,)\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset= training_set,\n",
    "    eval_dataset=validation_set,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cf108d7-6ec0-4ee8-8c71-15accf8ceb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [626/626 05:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.356935</td>\n",
       "      <td>0.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.333106</td>\n",
       "      <td>0.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.314133</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>1.368475</td>\n",
       "      <td>1.301800</td>\n",
       "      <td>0.649000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/people/dicko/miniconda3/envs/DataSR/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=626, training_loss=1.361582064400085, metrics={'train_runtime': 329.1375, 'train_samples_per_second': 30.382, 'train_steps_per_second': 1.902, 'total_flos': 2632290263040000.0, 'train_loss': 1.361582064400085, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111faf3a-272b-4dea-9f9a-8b00e7e194a5",
   "metadata": {},
   "source": [
    "## Compare the results of the different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933b06b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
