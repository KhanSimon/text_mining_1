{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ba93fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
      "  Downloading SQLAlchemy-1.4.54-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "                                              0.0/1.6 MB ? eta -:--:--\n",
      "     ----------                               0.4/1.6 MB 8.9 MB/s eta 0:00:01\n",
      "     --------------------------------         1.3/1.6 MB 13.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 14.5 MB/s eta 0:00:00\n",
      "Collecting alembic>=0.6.2 (from dataset)\n",
      "  Downloading alembic-1.18.3-py3-none-any.whl (262 kB)\n",
      "                                              0.0/262.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 262.3/262.3 kB ? eta 0:00:00\n",
      "Collecting banal>=1.0.1 (from dataset)\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Collecting Mako (from alembic>=0.6.2->dataset)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from alembic>=0.6.2->dataset) (4.15.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy<2.0.0,>=1.3.2->dataset)\n",
      "  Downloading greenlet-3.3.1-cp311-cp311-win_amd64.whl (226 kB)\n",
      "                                              0.0/226.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 226.5/226.5 kB 13.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from Mako->alembic>=0.6.2->dataset) (3.0.3)\n",
      "Installing collected packages: banal, Mako, greenlet, sqlalchemy, alembic, dataset\n",
      "Successfully installed Mako-1.3.10 alembic-1.18.3 banal-1.0.6 dataset-1.6.2 greenlet-3.3.1 sqlalchemy-1.4.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "                                              0.0/515.2 kB ? eta -:--:--\n",
      "     ------------------------------------  512.0/515.2 kB 16.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 515.2/515.2 kB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-23.0.0-cp311-cp311-win_amd64.whl (27.5 MB)\n",
      "                                              0.0/27.5 MB ? eta -:--:--\n",
      "     -                                        1.3/27.5 MB 42.6 MB/s eta 0:00:01\n",
      "     ---                                      2.6/27.5 MB 33.3 MB/s eta 0:00:01\n",
      "     -----                                    3.8/27.5 MB 30.3 MB/s eta 0:00:01\n",
      "     -------                                  4.9/27.5 MB 28.6 MB/s eta 0:00:01\n",
      "     ---------                                6.3/27.5 MB 28.8 MB/s eta 0:00:01\n",
      "     -----------                              8.0/27.5 MB 30.3 MB/s eta 0:00:01\n",
      "     ------------                             8.9/27.5 MB 28.6 MB/s eta 0:00:01\n",
      "     --------------                          10.5/27.5 MB 28.5 MB/s eta 0:00:01\n",
      "     -----------------                       12.5/27.5 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------                     13.9/27.5 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------                  15.6/27.5 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------                17.5/27.5 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------             19.1/27.5 MB 36.4 MB/s eta 0:00:01\n",
      "     -----------------------------           20.7/27.5 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------         22.1/27.5 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------        22.9/27.5 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------      24.1/27.5 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------      24.3/27.5 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------    25.5/27.5 MB 27.3 MB/s eta 0:00:01\n",
      "     -------------------------------------   26.5/27.5 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  27.5/27.5 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  27.5/27.5 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  27.5/27.5 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 27.5/27.5 MB 17.7 MB/s eta 0:00:00\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp311-cp311-win_amd64.whl (31 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Using cached multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Collecting fsspec[http]<=2025.10.0,>=2023.1.0 (from datasets)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.3-cp311-cp311-win_amd64.whl (457 kB)\n",
      "                                              0.0/457.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 457.7/457.7 kB 14.0 MB/s eta 0:00:00\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "                                              0.0/113.6 kB ? eta -:--:--\n",
      "     ---------------------------------------  112.6/113.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 113.6/113.6 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "                                              0.0/44.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.1/44.1 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.1-cp311-cp311-win_amd64.whl (45 kB)\n",
      "                                              0.0/46.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "                                              0.0/41.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.6/41.6 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "                                              0.0/86.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.9/86.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, h11, fsspec, frozenlist, dill, attrs, anyio, aiohappyeyeballs, yarl, multiprocess, httpcore, aiosignal, httpx, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2026.1.0\n",
      "    Uninstalling fsspec-2026.1.0:\n",
      "      Successfully uninstalled fsspec-2026.1.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 anyio-4.12.1 attrs-25.4.0 datasets-4.5.0 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 multidict-6.7.1 multiprocess-0.70.18 propcache-0.4.1 pyarrow-23.0.0 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e9b9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from gensim.models import Word2Vec\n",
    "# from seqeval.metrics import classification_report, f1_score\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, EvalPrediction, AutoModelForTokenClassification, Trainer\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import ast\n",
    "# from datasets import load_dataset, ClassLabel, Value, Sequence, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datasets import load_dataset, ClassLabel, Value, Sequence, load_from_disk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee203ae",
   "metadata": {},
   "source": [
    "### `CNN Classification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9464fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 128\n",
    "# EMBEDDING_DIM = 100\n",
    "# BATCH_SIZE = 32\n",
    "# EPOCHS = 100\n",
    "# LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# bert_model = args.model\n",
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "bert_model = \"camembert-base\"\n",
    "nb_labels = 2\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = \"w2v_med_cbow.model\"\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtrain_layer1_ID.csv\",\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/EMEA/EMEAtrain_layer1_ID.csv\",\n",
    "]\n",
    "VALID_FILES = [\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEdev_layer1_ID.csv\",\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/EMEA/EMEAdev_layer1_ID.csv\",\n",
    "]\n",
    "TEST_FILES = [\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtest_layer1_ID.csv\",\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/EMEA/EMEAtest_layer1_ID.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad480860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LOADING DATA ---\n",
      "Loading MEDLINEtrain_layer1_ID.csv... -> 91 sentences.\n",
      "Loading EMEAtrain_layer1_ID.csv... -> 120 sentences.\n",
      "Loading MEDLINEdev_layer1_ID.csv... -> 90 sentences.\n",
      "Loading EMEAdev_layer1_ID.csv... -> 106 sentences.\n",
      "Loading MEDLINEtest_layer1_ID.csv... -> 94 sentences.\n",
      "Loading EMEAtest_layer1_ID.csv... -> 97 sentences.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. ROBUST DATA LOADER ---\n",
    "def load_data_from_csv(file_paths):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "\n",
    "    for fpath in file_paths:\n",
    "        if not os.path.exists(fpath):\n",
    "            print(f\"‚ùå File not found: {fpath}\")\n",
    "            continue\n",
    "        print(f\"Loading {os.path.basename(fpath)}...\", end=\" \")\n",
    "\n",
    "        try:\n",
    "            # key: skip_blank_lines=False preserves sentence separators\n",
    "            df = pd.read_csv(\n",
    "                fpath,\n",
    "                sep=None,\n",
    "                engine=\"python\",\n",
    "                keep_default_na=False,\n",
    "                skip_blank_lines=False,\n",
    "            )\n",
    "        except:\n",
    "            print(\"Read Failed.\")\n",
    "            continue\n",
    "\n",
    "        # Detect columns\n",
    "        if \"Mot\" in df.columns and \"Tag\" in df.columns:\n",
    "            words, tags = df[\"Mot\"].astype(str).values, df[\"Tag\"].astype(str).values\n",
    "        else:\n",
    "            words, tags = (\n",
    "                df.iloc[:, 0].astype(str).values,\n",
    "                df.iloc[:, -1].astype(str).values,\n",
    "            )\n",
    "\n",
    "        curr_s, curr_t = [], []\n",
    "        file_s, file_t = [], []\n",
    "\n",
    "        for w, t in zip(words, tags):\n",
    "            if not w.strip():  # Empty line = Sentence Break\n",
    "                if curr_s:\n",
    "                    file_s.append(curr_s)\n",
    "                    file_t.append(curr_t)\n",
    "                    curr_s, curr_t = [], []\n",
    "            else:\n",
    "                curr_s.append(w)\n",
    "                curr_t.append(t)\n",
    "        if curr_s:\n",
    "            file_s.append(curr_s)\n",
    "            file_t.append(curr_t)\n",
    "\n",
    "        # Fallback for giant files (Chunking)\n",
    "        if len(file_s) < 10 and len(words) > 500:\n",
    "            flat_w = [w for s in file_s for w in s]\n",
    "            flat_t = [t for s in file_t for t in s]\n",
    "            file_s = [\n",
    "                flat_w[i : i + SEQUENCE_LENGTH]\n",
    "                for i in range(0, len(flat_w), SEQUENCE_LENGTH)\n",
    "            ]\n",
    "            file_t = [\n",
    "                flat_t[i : i + SEQUENCE_LENGTH]\n",
    "                for i in range(0, len(flat_t), SEQUENCE_LENGTH)\n",
    "            ]\n",
    "\n",
    "        print(f\"-> {len(file_s)} sentences.\")\n",
    "        all_sentences.extend(file_s)\n",
    "        all_tags.extend(file_t)\n",
    "\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "\n",
    "print(\"--- LOADING DATA ---\")\n",
    "train_sents, train_tags = load_data_from_csv(TRAIN_FILES)\n",
    "valid_sents, valid_tags = load_data_from_csv(VALID_FILES)\n",
    "test_sents, test_tags = load_data_from_csv(TEST_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687b3753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INITIALIZING EMBEDDINGS ---\n",
      "Vocab Size: 5775\n",
      "Pre-trained Weights Found: 5615/5775 (97.2%)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. BUILD VOCAB & INITIALIZE EMBEDDINGS (TP1) ---\n",
    "print(\"\\n--- INITIALIZING EMBEDDINGS ---\")\n",
    "\n",
    "# A. Build Vocabulary from Training Data\n",
    "word_counts = {}\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word2idx = {w: i + 2 for i, w in enumerate(vocab)}  # Start at 2\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<UNK>\"] = 1\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "# B. Load Word2Vec and Create Weight Matrix\n",
    "w2v_model = Word2Vec.load(W2V_PATH)\n",
    "vocab_size = len(word2idx)\n",
    "embedding_matrix = np.zeros((vocab_size, MAX_LEN))\n",
    "hits = 0\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "        hits += 1\n",
    "    elif word.lower() in w2v_model.wv:  # Case Insensitive Fallback\n",
    "        embedding_matrix[idx] = w2v_model.wv[word.lower()]\n",
    "        hits += 1\n",
    "    else:\n",
    "        # Initialize OOV words with random noise\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(MAX_LEN,))\n",
    "\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Pre-trained Weights Found: {hits}/{vocab_size} ({hits/vocab_size:.1%})\")\n",
    "\n",
    "# Convert to FloatTensor\n",
    "embedding_weights = torch.tensor(embedding_matrix, dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d60b213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: {'B-ANAT': 1, 'B-CHEM': 2, 'B-DEVI': 3, 'B-DISO': 4, 'B-GEOG': 5, 'B-LIVB': 6, 'B-OBJC': 7, 'B-PHEN': 8, 'B-PHYS': 9, 'B-PROC': 10, 'I-ANAT': 11, 'I-CHEM': 12, 'I-DEVI': 13, 'I-DISO': 14, 'I-GEOG': 15, 'I-LIVB': 16, 'I-OBJC': 17, 'I-PHEN': 18, 'I-PHYS': 19, 'I-PROC': 20, 'O': 21, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. DATA PREPARATION (ENCODING) ---\n",
    "def encode_sequences(seqs, mapping, default_val, max_len=128):\n",
    "    encoded = []\n",
    "    for s in seqs:\n",
    "        seq = [mapping.get(item, default_val) for item in s]\n",
    "        if len(seq) < max_len:\n",
    "            seq += [0] * (max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:max_len]\n",
    "        encoded.append(seq)\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Encode Words (Input)\n",
    "X_train = encode_sequences(train_sents, word2idx, 1)  # 1 = UNK\n",
    "X_valid = encode_sequences(valid_sents, word2idx, 1)\n",
    "X_test = encode_sequences(test_sents, word2idx, 1)\n",
    "\n",
    "# Encode Tags (Target)\n",
    "tag_set = set(t for s in train_tags + valid_tags + test_tags for t in s)\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(sorted(list(tag_set)))}\n",
    "tag2idx[\"<PAD>\"] = 0\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "print(f\"Tags: {tag2idx}\")\n",
    "\n",
    "y_train = encode_sequences(train_tags, tag2idx, 0)  # 0 = PAD (ignored in loss)\n",
    "y_valid = encode_sequences(valid_tags, tag2idx, 0)\n",
    "y_test = encode_sequences(test_tags, tag2idx, 0)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, y_train), shuffle=True, batch_size=TRAIN_BATCH_SIZE\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    TensorDataset(X_valid, y_valid), shuffle=False, batch_size=VALID_BATCH_SIZE\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test, y_test), shuffle=False, batch_size=VALID_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "388e0283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight True\n",
      "roberta.embeddings.position_embeddings.weight True\n",
      "roberta.embeddings.token_type_embeddings.weight True\n",
      "roberta.embeddings.LayerNorm.weight True\n",
      "roberta.embeddings.LayerNorm.bias True\n",
      "roberta.encoder.layer.0.attention.self.query.weight True\n",
      "roberta.encoder.layer.0.attention.self.query.bias True\n",
      "roberta.encoder.layer.0.attention.self.key.weight True\n",
      "roberta.encoder.layer.0.attention.self.key.bias True\n",
      "roberta.encoder.layer.0.attention.self.value.weight True\n",
      "roberta.encoder.layer.0.attention.self.value.bias True\n",
      "roberta.encoder.layer.0.attention.output.dense.weight True\n",
      "roberta.encoder.layer.0.attention.output.dense.bias True\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.0.intermediate.dense.weight True\n",
      "roberta.encoder.layer.0.intermediate.dense.bias True\n",
      "roberta.encoder.layer.0.output.dense.weight True\n",
      "roberta.encoder.layer.0.output.dense.bias True\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.1.attention.self.query.weight True\n",
      "roberta.encoder.layer.1.attention.self.query.bias True\n",
      "roberta.encoder.layer.1.attention.self.key.weight True\n",
      "roberta.encoder.layer.1.attention.self.key.bias True\n",
      "roberta.encoder.layer.1.attention.self.value.weight True\n",
      "roberta.encoder.layer.1.attention.self.value.bias True\n",
      "roberta.encoder.layer.1.attention.output.dense.weight True\n",
      "roberta.encoder.layer.1.attention.output.dense.bias True\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.1.intermediate.dense.weight True\n",
      "roberta.encoder.layer.1.intermediate.dense.bias True\n",
      "roberta.encoder.layer.1.output.dense.weight True\n",
      "roberta.encoder.layer.1.output.dense.bias True\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.2.attention.self.query.weight True\n",
      "roberta.encoder.layer.2.attention.self.query.bias True\n",
      "roberta.encoder.layer.2.attention.self.key.weight True\n",
      "roberta.encoder.layer.2.attention.self.key.bias True\n",
      "roberta.encoder.layer.2.attention.self.value.weight True\n",
      "roberta.encoder.layer.2.attention.self.value.bias True\n",
      "roberta.encoder.layer.2.attention.output.dense.weight True\n",
      "roberta.encoder.layer.2.attention.output.dense.bias True\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.2.intermediate.dense.weight True\n",
      "roberta.encoder.layer.2.intermediate.dense.bias True\n",
      "roberta.encoder.layer.2.output.dense.weight True\n",
      "roberta.encoder.layer.2.output.dense.bias True\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.3.attention.self.query.weight True\n",
      "roberta.encoder.layer.3.attention.self.query.bias True\n",
      "roberta.encoder.layer.3.attention.self.key.weight True\n",
      "roberta.encoder.layer.3.attention.self.key.bias True\n",
      "roberta.encoder.layer.3.attention.self.value.weight True\n",
      "roberta.encoder.layer.3.attention.self.value.bias True\n",
      "roberta.encoder.layer.3.attention.output.dense.weight True\n",
      "roberta.encoder.layer.3.attention.output.dense.bias True\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.3.intermediate.dense.weight True\n",
      "roberta.encoder.layer.3.intermediate.dense.bias True\n",
      "roberta.encoder.layer.3.output.dense.weight True\n",
      "roberta.encoder.layer.3.output.dense.bias True\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.4.attention.self.query.weight True\n",
      "roberta.encoder.layer.4.attention.self.query.bias True\n",
      "roberta.encoder.layer.4.attention.self.key.weight True\n",
      "roberta.encoder.layer.4.attention.self.key.bias True\n",
      "roberta.encoder.layer.4.attention.self.value.weight True\n",
      "roberta.encoder.layer.4.attention.self.value.bias True\n",
      "roberta.encoder.layer.4.attention.output.dense.weight True\n",
      "roberta.encoder.layer.4.attention.output.dense.bias True\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.4.intermediate.dense.weight True\n",
      "roberta.encoder.layer.4.intermediate.dense.bias True\n",
      "roberta.encoder.layer.4.output.dense.weight True\n",
      "roberta.encoder.layer.4.output.dense.bias True\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.5.attention.self.query.weight True\n",
      "roberta.encoder.layer.5.attention.self.query.bias True\n",
      "roberta.encoder.layer.5.attention.self.key.weight True\n",
      "roberta.encoder.layer.5.attention.self.key.bias True\n",
      "roberta.encoder.layer.5.attention.self.value.weight True\n",
      "roberta.encoder.layer.5.attention.self.value.bias True\n",
      "roberta.encoder.layer.5.attention.output.dense.weight True\n",
      "roberta.encoder.layer.5.attention.output.dense.bias True\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.5.intermediate.dense.weight True\n",
      "roberta.encoder.layer.5.intermediate.dense.bias True\n",
      "roberta.encoder.layer.5.output.dense.weight True\n",
      "roberta.encoder.layer.5.output.dense.bias True\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.6.attention.self.query.weight True\n",
      "roberta.encoder.layer.6.attention.self.query.bias True\n",
      "roberta.encoder.layer.6.attention.self.key.weight True\n",
      "roberta.encoder.layer.6.attention.self.key.bias True\n",
      "roberta.encoder.layer.6.attention.self.value.weight True\n",
      "roberta.encoder.layer.6.attention.self.value.bias True\n",
      "roberta.encoder.layer.6.attention.output.dense.weight True\n",
      "roberta.encoder.layer.6.attention.output.dense.bias True\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.6.intermediate.dense.weight True\n",
      "roberta.encoder.layer.6.intermediate.dense.bias True\n",
      "roberta.encoder.layer.6.output.dense.weight True\n",
      "roberta.encoder.layer.6.output.dense.bias True\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.7.attention.self.query.weight True\n",
      "roberta.encoder.layer.7.attention.self.query.bias True\n",
      "roberta.encoder.layer.7.attention.self.key.weight True\n",
      "roberta.encoder.layer.7.attention.self.key.bias True\n",
      "roberta.encoder.layer.7.attention.self.value.weight True\n",
      "roberta.encoder.layer.7.attention.self.value.bias True\n",
      "roberta.encoder.layer.7.attention.output.dense.weight True\n",
      "roberta.encoder.layer.7.attention.output.dense.bias True\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.7.intermediate.dense.weight True\n",
      "roberta.encoder.layer.7.intermediate.dense.bias True\n",
      "roberta.encoder.layer.7.output.dense.weight True\n",
      "roberta.encoder.layer.7.output.dense.bias True\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.8.attention.self.query.weight True\n",
      "roberta.encoder.layer.8.attention.self.query.bias True\n",
      "roberta.encoder.layer.8.attention.self.key.weight True\n",
      "roberta.encoder.layer.8.attention.self.key.bias True\n",
      "roberta.encoder.layer.8.attention.self.value.weight True\n",
      "roberta.encoder.layer.8.attention.self.value.bias True\n",
      "roberta.encoder.layer.8.attention.output.dense.weight True\n",
      "roberta.encoder.layer.8.attention.output.dense.bias True\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.8.intermediate.dense.weight True\n",
      "roberta.encoder.layer.8.intermediate.dense.bias True\n",
      "roberta.encoder.layer.8.output.dense.weight True\n",
      "roberta.encoder.layer.8.output.dense.bias True\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.9.attention.self.query.weight True\n",
      "roberta.encoder.layer.9.attention.self.query.bias True\n",
      "roberta.encoder.layer.9.attention.self.key.weight True\n",
      "roberta.encoder.layer.9.attention.self.key.bias True\n",
      "roberta.encoder.layer.9.attention.self.value.weight True\n",
      "roberta.encoder.layer.9.attention.self.value.bias True\n",
      "roberta.encoder.layer.9.attention.output.dense.weight True\n",
      "roberta.encoder.layer.9.attention.output.dense.bias True\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.9.intermediate.dense.weight True\n",
      "roberta.encoder.layer.9.intermediate.dense.bias True\n",
      "roberta.encoder.layer.9.output.dense.weight True\n",
      "roberta.encoder.layer.9.output.dense.bias True\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.10.attention.self.query.weight True\n",
      "roberta.encoder.layer.10.attention.self.query.bias True\n",
      "roberta.encoder.layer.10.attention.self.key.weight True\n",
      "roberta.encoder.layer.10.attention.self.key.bias True\n",
      "roberta.encoder.layer.10.attention.self.value.weight True\n",
      "roberta.encoder.layer.10.attention.self.value.bias True\n",
      "roberta.encoder.layer.10.attention.output.dense.weight True\n",
      "roberta.encoder.layer.10.attention.output.dense.bias True\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.10.intermediate.dense.weight True\n",
      "roberta.encoder.layer.10.intermediate.dense.bias True\n",
      "roberta.encoder.layer.10.output.dense.weight True\n",
      "roberta.encoder.layer.10.output.dense.bias True\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.11.attention.self.query.weight True\n",
      "roberta.encoder.layer.11.attention.self.query.bias True\n",
      "roberta.encoder.layer.11.attention.self.key.weight True\n",
      "roberta.encoder.layer.11.attention.self.key.bias True\n",
      "roberta.encoder.layer.11.attention.self.value.weight True\n",
      "roberta.encoder.layer.11.attention.self.value.bias True\n",
      "roberta.encoder.layer.11.attention.output.dense.weight True\n",
      "roberta.encoder.layer.11.attention.output.dense.bias True\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.11.intermediate.dense.weight True\n",
      "roberta.encoder.layer.11.intermediate.dense.bias True\n",
      "roberta.encoder.layer.11.output.dense.weight True\n",
      "roberta.encoder.layer.11.output.dense.bias True\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(bert_model, num_labels=nb_labels)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"finetune_\" + bert_model.replace(\"/\",\"_\") + \"_\" + str(EPOCHS) + \"_\" + str(TRAIN_BATCH_SIZE),\n",
    "    save_strategy = \"epoch\",\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    load_best_model_at_end=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "120d6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for dataset preprocessing\n",
    "def string_list_2_list(x):\n",
    "    if type(x[\"Tag\"][0]) == list:\n",
    "        x[\"Tag\"] = list(map(ast.literal_eval,x[\"Tag\"]))\n",
    "    return x\n",
    "\n",
    "def encode(examples):\n",
    "#    print (examples) \n",
    "    r = tokenizer(list(map(str, examples['review'])), padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    #r = tokenizer(list(map(str, examples['text'])), padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    return r\n",
    "\n",
    "def preprocess_labels(examples):\n",
    "    if type(examples['Tag'][0]) == list:\n",
    "        examples['Tag'] = [[float(x) for x in label] for label in examples['Tag']]\n",
    "\n",
    "    else:\n",
    "        examples['Tag'] = [float(x) for x in examples['Tag']]\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    precision_weighted_average = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_average = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    f1_weighted_average = 2.0 * ((precision_weighted_average * recall_average) / ( precision_weighted_average + recall_average ))\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(classification_report(y_pred, y_true))\n",
    "    # return as dictionary\n",
    "    metrics = {'precision weighted': precision_weighted_average,\n",
    "               'recall weighted': recall_average,\n",
    "               'f1 weighted': f1_weighted_average,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def multi_label_metrics_v2(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    return (classification_report(y_true, y_pred))\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(predictions=preds,labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "def padding(examples):\n",
    "    for key in ['input_ids', 'attention_mask']:\n",
    "        for i, element in enumerate(examples[key]):\n",
    "            if len(element) < 20:\n",
    "                for i in range(20 - len(element)):\n",
    "                    element.append(0)\n",
    "            else:\n",
    "                examples[key][i] = element[:20]\n",
    "    return examples\n",
    "\n",
    "def preprocess_dataset(csvfile, binfile, kind='train'):\n",
    "    l_data = load_dataset('csv', data_files={kind: [csvfile]})\n",
    "    #data = load_dataset(csvfile, split=kind)\n",
    "    l_data = l_data.map(string_list_2_list, batched=True)\n",
    "    l_data = l_data.map(preprocess_labels, batched=True)\n",
    "    l_data = l_data.map(encode, batched=True)\n",
    "    new_features = l_data[kind].features.copy()\n",
    "    print(new_features)\n",
    "    if type(l_data[kind][\"Tag\"][0]) == list:\n",
    "        new_features[\"Tag\"] = Sequence(feature=Value(dtype='float'))\n",
    "    else:\n",
    "        new_features[\"Tag\"] = feature=Value(dtype='float')\n",
    "    print(new_features)\n",
    "    l_data = l_data[kind].cast(new_features)\n",
    "    l_data = l_data.map(padding, batched=True)\n",
    "    l_data.save_to_disk(binfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ca2145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/11525 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'B-PROC'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m test_binary = \u001b[33m'\u001b[39m\u001b[33mTP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtest_layer1_ID.csv\u001b[39m\u001b[33m'\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m + bert_model.replace(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(MAX_LEN) + \u001b[33m\"\u001b[39m\u001b[33m.bin\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(train_binary) :\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtrain_layer1_ID.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(valid_binary) :\n\u001b[32m      9\u001b[39m     preprocess_dataset(\u001b[33m'\u001b[39m\u001b[33mTP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEdev_layer1_ID.csv\u001b[39m\u001b[33m'\u001b[39m, valid_binary, kind=\u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mpreprocess_dataset\u001b[39m\u001b[34m(csvfile, binfile, kind)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m#data = load_dataset(csvfile, split=kind)\u001b[39;00m\n\u001b[32m     78\u001b[39m l_data = l_data.map(string_list_2_list, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m l_data = \u001b[43ml_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m l_data = l_data.map(encode, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     81\u001b[39m new_features = l_data[kind].features.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\datasets\\dataset_dict.py:953\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    951\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    975\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\datasets\\arrow_dataset.py:3343\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3341\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3342\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3343\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\datasets\\arrow_dataset.py:3699\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3697\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3698\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3699\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3701\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\datasets\\arrow_dataset.py:3649\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3648\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3649\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\datasets\\arrow_dataset.py:3572\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3570\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3571\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3572\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mpreprocess_labels\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     15\u001b[39m     examples[\u001b[33m'\u001b[39m\u001b[33mTag\u001b[39m\u001b[33m'\u001b[39m] = [[\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[33m'\u001b[39m\u001b[33mTag\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     examples[\u001b[33m'\u001b[39m\u001b[33mTag\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43m[\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTag\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m examples\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     15\u001b[39m     examples[\u001b[33m'\u001b[39m\u001b[33mTag\u001b[39m\u001b[33m'\u001b[39m] = [[\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[33m'\u001b[39m\u001b[33mTag\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     examples[\u001b[33m'\u001b[39m\u001b[33mTag\u001b[39m\u001b[33m'\u001b[39m] = [\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[33m'\u001b[39m\u001b[33mTag\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m examples\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'B-PROC'"
     ]
    }
   ],
   "source": [
    "train_binary = 'TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtrain_layer1_ID.csv' + \"_\" + bert_model.replace(\"/\",\"_\") + \"_\" + str(MAX_LEN) + \".bin\"\n",
    "valid_binary = 'TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEdev_layer1_ID.csv' + \"_\" + bert_model.replace(\"/\",\"_\") + \"_\" + str(MAX_LEN) + \".bin\"\n",
    "test_binary = 'TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtest_layer1_ID.csv' + \"_\" + bert_model.replace(\"/\",\"_\") + \"_\" + str(MAX_LEN) + \".bin\"\n",
    "\n",
    "\n",
    "if not os.path.exists(train_binary) :\n",
    "    preprocess_dataset('TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtrain_layer1_ID.csv', train_binary, kind='train')\n",
    "if not os.path.exists(valid_binary) :\n",
    "    preprocess_dataset('TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEdev_layer1_ID.csv', valid_binary, kind='valid')\n",
    "if not os.path.exists(test_binary) :\n",
    "    preprocess_dataset('TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtest_layer1_ID.csv', test_binary, kind='test')\n",
    "\n",
    "\n",
    "\n",
    "data_train = load_from_disk(train_binary)\n",
    "data_test = load_from_disk(test_binary)\n",
    "data_valid = load_from_disk(valid_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28085613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyler Marino\\AppData\\Local\\Temp\\ipykernel_41712\\2989772325.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, args=training_args, train_dataset=X_train, eval_dataset=y_train, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
      "c:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(model=model, args=training_args, train_dataset=X_train, eval_dataset=y_train, tokenizer=tokenizer, compute_metrics=compute_metrics)\n\u001b[32m      2\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m end = time.time()\n\u001b[32m      5\u001b[39m elapsed = end - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\transformers\\trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\transformers\\trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\accelerate\\data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\transformers\\trainer_utils.py:872\u001b[39m, in \u001b[36mRemoveColumnsCollator.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    870\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[32m    871\u001b[39m     features = [\u001b[38;5;28mself\u001b[39m._remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[32m--> \u001b[39m\u001b[32m872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\transformers\\data\\data_collator.py:271\u001b[39m, in \u001b[36mDataCollatorWithPadding.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    280\u001b[39m         batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     69\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3516\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3512\u001b[39m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has been passed for padding\u001b[39;00m\n\u001b[32m   3513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[32m   3514\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3515\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3516\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[43mencoded_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   3517\u001b[39m     )\n\u001b[32m   3519\u001b[39m required_input = encoded_inputs[\u001b[38;5;28mself\u001b[39m.model_input_names[\u001b[32m0\u001b[39m]]\n\u001b[32m   3521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) == \u001b[32m0\u001b[39m):\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=X_train, eval_dataset=y_train, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(\"************** RESULTS for \" + bert_model.replace(\"/\",\"_\") + \" *****************\")\n",
    "print(\"Training time: \" + str(elapsed) + \" ms\")\n",
    "results_valid = trainer.evaluate(valid_loader)\n",
    "print(results_valid)\n",
    "results_test = trainer.evaluate(test_loader)\n",
    "print(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f03cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST RESULTS ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ANAT       0.38      0.24      0.30       364\n",
      "        CHEM       0.45      0.19      0.27      1037\n",
      "        DEVI       0.15      0.04      0.06       107\n",
      "        DISO       0.35      0.24      0.29       977\n",
      "        GEOG       0.50      0.05      0.09        63\n",
      "        LIVB       0.75      0.57      0.65       498\n",
      "        OBJC       0.14      0.04      0.06        81\n",
      "        PHEN       0.00      0.00      0.00        70\n",
      "        PHYS       0.48      0.21      0.29       190\n",
      "        PROC       0.41      0.49      0.45       761\n",
      "\n",
      "   micro avg       0.44      0.30      0.35      4148\n",
      "   macro avg       0.36      0.21      0.24      4148\n",
      "weighted avg       0.43      0.30      0.34      4148\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# --- 7. FINAL TEST EVALUATION ---\n",
    "print(\"\\n--- TEST RESULTS ---\")\n",
    "model.load_state_dict(torch.load(\"best_cnn_ner.pt\"))\n",
    "model.eval()\n",
    "test_true, test_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        out = model(x)\n",
    "        preds = torch.argmax(out, dim=2).cpu().numpy()\n",
    "        labels = y.numpy()\n",
    "        for i in range(len(x)):\n",
    "            p_s, t_s = [], []\n",
    "            for j in range(SEQUENCE_LENGTH):\n",
    "                if labels[i][j] == 0:\n",
    "                    break\n",
    "                p_s.append(idx2tag[preds[i][j]])\n",
    "                t_s.append(idx2tag[labels[i][j]])\n",
    "            test_pred.append(p_s)\n",
    "            test_true.append(t_s)\n",
    "\n",
    "print(classification_report(test_true, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c4a19",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "527c3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c85124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SWITCHING TO Bi-LSTM ---\n",
      "Vocab Size: 5775\n",
      "Embedding Dim: 100\n"
     ]
    }
   ],
   "source": [
    "# --- 2. CONFIGURATION ---\n",
    "HIDDEN_DIM = 256\n",
    "LSTM_EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"--- SWITCHING TO Bi-LSTM ---\")\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Embedding Dim: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87059be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Bi-LSTM MODEL DEFINITION ---\n",
    "class LSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, pretrained_weights):\n",
    "        super(LSTM_NER, self).__init__()\n",
    "\n",
    "        # 1. Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Initialize with TP1 Weights (Same as CNN)\n",
    "        self.embedding.weight.data.copy_(pretrained_weights)\n",
    "        # Optional: Unfreeze to allow fine-tuning\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        # 2. Bi-LSTM Layer\n",
    "        # batch_first=True -> Input is (Batch, Seq, Feature)\n",
    "        # bidirectional=True -> Output is Hidden * 2\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # 3. Fully Connected Layer\n",
    "        # Maps from [Hidden * 2] to [Num_Tags]\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, Seq_Len]\n",
    "\n",
    "        # Get Embeddings\n",
    "        embeds = self.embedding(x)\n",
    "        # embeds shape: [Batch, Seq_Len, Embed_Dim]\n",
    "\n",
    "        # LSTM Pass\n",
    "        # No permutation needed here (LSTM expects Batch, Seq, Dim)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # lstm_out shape: [Batch, Seq_Len, Hidden_Dim * 2]\n",
    "\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        # Projection to Tag Space\n",
    "        logits = self.fc(lstm_out)\n",
    "        # logits shape: [Batch, Seq_Len, Num_Classes]\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Initialize Model\n",
    "model = LSTM_NER(vocab_size, EMBEDDING_DIM, HIDDEN_DIM,\n",
    "                 len(tag2idx), embedding_weights).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f717fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(len(tag2idx)).to(DEVICE)\n",
    "if 'O' in tag2idx:\n",
    "    weights[tag2idx['O']] = 0.5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, ignore_index=0)\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9241ca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STARTING Bi-LSTM TRAINING ON cpu ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LSTM_EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- STARTING Bi-LSTM TRAINING ON \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m best_f1 = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mLSTM_EPOCHS\u001b[49m):\n\u001b[32m      5\u001b[39m     model.train()\n\u001b[32m      6\u001b[39m     train_loss = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'LSTM_EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- STARTING Bi-LSTM TRAINING ON {DEVICE} ---\")\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(LSTM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Training Step\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)  # [Batch, Seq, Num_Classes]\n",
    "\n",
    "        # Flatten for Loss: (Batch * Seq, Num_Classes) vs (Batch * Seq)\n",
    "        loss = criterion(out.view(-1, len(tag2idx)), y.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation Step\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "\n",
    "            # Get Predictions\n",
    "            preds = torch.argmax(out, dim=2).cpu().numpy()\n",
    "            labels = y.cpu().numpy()\n",
    "\n",
    "            # Decode (Remove Padding)\n",
    "            for i in range(len(x)):\n",
    "                p_s, t_s = [], []\n",
    "                for j in range(SEQUENCE_LENGTH):\n",
    "                    if labels[i][j] == 0:\n",
    "                        break  # Stop at padding\n",
    "                    p_s.append(idx2tag[preds[i][j]])\n",
    "                    t_s.append(idx2tag[labels[i][j]])\n",
    "                all_pred.append(p_s)\n",
    "                all_true.append(t_s)\n",
    "\n",
    "    # Metrics\n",
    "    val_f1 = f1_score(all_true, all_pred)\n",
    "    print(f\"Loss: {train_loss/len(train_loader):.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save Best\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_lstm_ner.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37b31b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bi-LSTM TEST RESULTS ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ANAT       0.34      0.13      0.19       364\n",
      "        CHEM       0.26      0.22      0.24      1037\n",
      "        DEVI       0.50      0.01      0.02       107\n",
      "        DISO       0.41      0.28      0.33       977\n",
      "        GEOG       0.00      0.00      0.00        63\n",
      "        LIVB       0.75      0.52      0.62       498\n",
      "        OBJC       0.00      0.00      0.00        81\n",
      "        PHEN       0.00      0.00      0.00        70\n",
      "        PHYS       0.38      0.16      0.22       190\n",
      "        PROC       0.50      0.46      0.48       761\n",
      "\n",
      "   micro avg       0.42      0.29      0.34      4148\n",
      "   macro avg       0.31      0.18      0.21      4148\n",
      "weighted avg       0.40      0.29      0.33      4148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Bi-LSTM TEST RESULTS ---\")\n",
    "model.load_state_dict(torch.load(\"best_lstm_ner.pt\"))\n",
    "model.eval()\n",
    "test_true, test_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        out = model(x)\n",
    "        preds = torch.argmax(out, dim=2).cpu().numpy()\n",
    "        labels = y.numpy()\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            p_s, t_s = [], []\n",
    "            for j in range(SEQUENCE_LENGTH):\n",
    "                if labels[i][j] == 0:\n",
    "                    break\n",
    "                p_s.append(idx2tag[preds[i][j]])\n",
    "                t_s.append(idx2tag[labels[i][j]])\n",
    "            test_pred.append(p_s)\n",
    "            test_true.append(t_s)\n",
    "\n",
    "print(classification_report(test_true, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367cb0c",
   "metadata": {},
   "source": [
    "### `French Press dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94578fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6a2e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "SEQUENCE_LENGTH = 128\n",
    "EMBEDDING_DIM = 100   # Must match your w2v_press_cbow.model\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# PATHS (Press Data & Press Embeddings)\n",
    "W2V_PATH = \"w2v_press_cbow.model\"\n",
    "PRESS_DIR = \"TP_ISD2020/QUAERO_FrenchPress\"\n",
    "\n",
    "TRAIN_FILES = [f\"{PRESS_DIR}/fra4_ID_train.csv\"]\n",
    "VALID_FILES = [f\"{PRESS_DIR}/fra4_ID_dev.csv\"]\n",
    "TEST_FILES = [f\"{PRESS_DIR}/fra4_ID_test.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63be61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LOADING PRESS DATA ---\n",
      "Loading fra4_ID_train.csv... -> 9034 sentences.\n",
      "Loading fra4_ID_dev.csv... -> 744 sentences.\n",
      "Loading fra4_ID_test.csv... -> 749 sentences.\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_csv(file_paths):\n",
    "    all_sentences, all_tags = [], []\n",
    "    for fpath in file_paths:\n",
    "        if not os.path.exists(fpath):\n",
    "            continue\n",
    "        print(f\"Loading {os.path.basename(fpath)}...\", end=\" \")\n",
    "        try:\n",
    "            df = pd.read_csv(fpath, sep=None, engine=\"python\",\n",
    "                             keep_default_na=False, skip_blank_lines=False)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if \"Mot\" in df.columns:\n",
    "            words, tags = df[\"Mot\"].astype(\n",
    "                str).values, df[\"Tag\"].astype(str).values\n",
    "        else:\n",
    "            words, tags = df.iloc[:, 0].astype(\n",
    "                str).values, df.iloc[:, -1].astype(str).values\n",
    "\n",
    "        curr_s, curr_t, file_s, file_t = [], [], [], []\n",
    "        for w, t in zip(words, tags):\n",
    "            if not w.strip():\n",
    "                if curr_s:\n",
    "                    file_s.append(curr_s)\n",
    "                    file_t.append(curr_t)\n",
    "                    curr_s, curr_t = [], []\n",
    "            else:\n",
    "                curr_s.append(w)\n",
    "                curr_t.append(t)\n",
    "        if curr_s:\n",
    "            file_s.append(curr_s)\n",
    "            file_t.append(curr_t)\n",
    "\n",
    "        # Chunking\n",
    "        if len(file_s) < 10 and len(words) > 500:\n",
    "            flat_w = [w for s in file_s for w in s]\n",
    "            flat_t = [t for s in file_t for t in s]\n",
    "            file_s = [flat_w[i:i+SEQUENCE_LENGTH]\n",
    "                      for i in range(0, len(flat_w), SEQUENCE_LENGTH)]\n",
    "            file_t = [flat_t[i:i+SEQUENCE_LENGTH]\n",
    "                      for i in range(0, len(flat_t), SEQUENCE_LENGTH)]\n",
    "\n",
    "        print(f\"-> {len(file_s)} sentences.\")\n",
    "        all_sentences.extend(file_s)\n",
    "        all_tags.extend(file_t)\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "\n",
    "print(\"--- LOADING PRESS DATA ---\")\n",
    "train_sents, train_tags = load_data_from_csv(TRAIN_FILES)\n",
    "valid_sents, valid_tags = load_data_from_csv(VALID_FILES)\n",
    "test_sents, test_tags = load_data_from_csv(TEST_FILES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bbf79d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING PRESS EMBEDDINGS ---\n",
      "Vocab: 37865 | Coverage: 99.5%\n"
     ]
    }
   ],
   "source": [
    "# --- 3. PREPARE EMBEDDINGS (PRESS W2V) ---\n",
    "print(\"\\n--- LOADING PRESS EMBEDDINGS ---\")\n",
    "# Build Vocab\n",
    "word_counts = {}\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word2idx = {w: i+2 for i, w in enumerate(vocab)}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "# Load Weights\n",
    "w2v_model = Word2Vec.load(W2V_PATH)\n",
    "embedding_matrix = np.zeros((len(word2idx), EMBEDDING_DIM))\n",
    "hits = 0\n",
    "for word, idx in word2idx.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "        hits += 1\n",
    "    elif word.lower() in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word.lower()]\n",
    "        hits += 1\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(\n",
    "            scale=0.6, size=(EMBEDDING_DIM,))\n",
    "\n",
    "print(f\"Vocab: {len(word2idx)} | Coverage: {hits/len(word2idx):.1%}\")\n",
    "embedding_weights = torch.tensor(\n",
    "    embedding_matrix, dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb5615e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: {'O': 1, 'b-func': 2, 'b-loc': 3, 'b-org': 4, 'b-pers': 5, 'b-prod': 6, 'i-func': 7, 'i-loc': 8, 'i-org': 9, 'i-pers': 10, 'i-prod': 11, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- 4. ENCODE DATA ---\n",
    "def encode_seq(seqs, mapping, default):\n",
    "    enc = []\n",
    "    for s in seqs:\n",
    "        r = [mapping.get(x, default) for x in s]\n",
    "        if len(r) < SEQUENCE_LENGTH:\n",
    "            r += [0]*(SEQUENCE_LENGTH-len(r))\n",
    "        else:\n",
    "            r = r[:SEQUENCE_LENGTH]\n",
    "        enc.append(r)\n",
    "    return torch.tensor(enc, dtype=torch.long)\n",
    "\n",
    "\n",
    "X_train = encode_seq(train_sents, word2idx, 1)\n",
    "X_valid = encode_seq(valid_sents, word2idx, 1)\n",
    "X_test = encode_seq(test_sents,  word2idx, 1)\n",
    "\n",
    "tag_set = set(t for s in train_tags+valid_tags+test_tags for t in s)\n",
    "tag2idx = {t: i+1 for i, t in enumerate(sorted(list(tag_set)))}\n",
    "tag2idx['<PAD>'] = 0\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "print(f\"Tags: {tag2idx}\")\n",
    "\n",
    "y_train = encode_seq(train_tags, tag2idx, 0)\n",
    "y_valid = encode_seq(valid_tags, tag2idx, 0)\n",
    "y_test = encode_seq(test_tags,  tag2idx, 0)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(\n",
    "    X_train, y_train), shuffle=True, batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(TensorDataset(\n",
    "    X_valid, y_valid), shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test),\n",
    "                         shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353604f",
   "metadata": {},
   "source": [
    "### `Part A : CNN Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "700e5460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " TRAINING CNN (PRESS)\n",
      "==============================\n",
      "Epoch 15 Complete\n",
      "--- CNN EVALUATION ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: b-prod seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i-prod seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: b-pers seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i-pers seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: b-org seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i-org seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: b-func seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: b-loc seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i-loc seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: i-func seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        func       0.53      0.47      0.50       624\n",
      "         loc       0.69      0.65      0.67       713\n",
      "         org       0.24      0.45      0.31       505\n",
      "        pers       0.79      0.63      0.70      1377\n",
      "        prod       0.03      0.40      0.06       335\n",
      "\n",
      "   micro avg       0.28      0.56      0.37      3554\n",
      "   macro avg       0.46      0.52      0.45      3554\n",
      "weighted avg       0.58      0.56      0.54      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*30 + \"\\n TRAINING CNN (PRESS)\\n\" + \"=\"*30)\n",
    "\n",
    "\n",
    "class CNN_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, weights):\n",
    "        super(CNN_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(weights)  # Init with Press W2V\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "        x = self.dropout(\n",
    "            F.relu(self.conv2(self.dropout(F.relu(self.conv1(x))))))\n",
    "        return self.fc(x.permute(0, 2, 1))\n",
    "\n",
    "\n",
    "model_cnn = CNN_NER(len(word2idx), EMBEDDING_DIM, len(\n",
    "    tag2idx), embedding_weights).to(DEVICE)\n",
    "weights = torch.ones(len(tag2idx)).to(DEVICE)\n",
    "if 'O' in tag2idx:\n",
    "    weights[tag2idx['O']] = 0.5\n",
    "opt = Adam(model_cnn.parameters(), lr=LEARNING_RATE)\n",
    "crit = nn.CrossEntropyLoss(weight=weights, ignore_index=0)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model_cnn.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        loss = crit(model_cnn(x).view(-1, len(tag2idx)), y.view(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    # Validation info\n",
    "    print(f\"Epoch {e+1} Complete\", end=\"\\r\")\n",
    "\n",
    "print(\"\\n--- CNN EVALUATION ---\")\n",
    "model_cnn.eval()\n",
    "test_true, test_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        out = model_cnn(x.to(DEVICE))\n",
    "        preds = torch.argmax(out, dim=2).cpu().numpy()\n",
    "        labels = y.numpy()\n",
    "        for i in range(len(x)):\n",
    "            p_s, t_s = [], []\n",
    "            for j in range(SEQUENCE_LENGTH):\n",
    "                if labels[i][j] == 0:\n",
    "                    break\n",
    "                p_s.append(idx2tag[preds[i][j]])\n",
    "                t_s.append(idx2tag[labels[i][j]])\n",
    "            test_pred.append(p_s)\n",
    "            test_true.append(t_s)\n",
    "print(classification_report(test_true, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840f79a",
   "metadata": {},
   "source": [
    "### `PART B: LSTM MODEL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2c30353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " TRAINING LSTM (PRESS)\n",
      "==============================\n",
      "Epoch 15 Complete\n",
      "--- LSTM EVALUATION ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        func       0.15      0.46      0.23       624\n",
      "         loc       0.69      0.65      0.66       713\n",
      "         org       0.24      0.44      0.31       505\n",
      "        pers       0.60      0.56      0.58      1377\n",
      "        prod       0.05      0.32      0.09       335\n",
      "\n",
      "   micro avg       0.27      0.52      0.35      3554\n",
      "   macro avg       0.35      0.49      0.37      3554\n",
      "weighted avg       0.44      0.52      0.45      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*30 + \"\\n TRAINING LSTM (PRESS)\\n\" + \"=\"*30)\n",
    "\n",
    "# Cleanup Memory\n",
    "del model_cnn, opt, crit\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "\n",
    "class LSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, weights):\n",
    "        super(LSTM_NER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(weights)  # Init with Press W2V\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        return self.fc(self.dropout(lstm_out))\n",
    "\n",
    "\n",
    "model_lstm = LSTM_NER(len(word2idx), EMBEDDING_DIM, 256,\n",
    "                      len(tag2idx), embedding_weights).to(DEVICE)\n",
    "opt = Adam(model_lstm.parameters(), lr=LEARNING_RATE)\n",
    "crit = nn.CrossEntropyLoss(weight=weights, ignore_index=0)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model_lstm.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        loss = crit(model_lstm(x).view(-1, len(tag2idx)), y.view(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(f\"Epoch {e+1} Complete\", end=\"\\r\")\n",
    "\n",
    "print(\"\\n--- LSTM EVALUATION ---\")\n",
    "model_lstm.eval()\n",
    "test_true, test_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        out = model_lstm(x.to(DEVICE))\n",
    "        preds = torch.argmax(out, dim=2).cpu().numpy()\n",
    "        labels = y.numpy()\n",
    "        for i in range(len(x)):\n",
    "            p_s, t_s = [], []\n",
    "            for j in range(SEQUENCE_LENGTH):\n",
    "                if labels[i][j] == 0:\n",
    "                    break\n",
    "                p_s.append(idx2tag[preds[i][j]])\n",
    "                t_s.append(idx2tag[labels[i][j]])\n",
    "            test_pred.append(p_s)\n",
    "            test_true.append(t_s)\n",
    "print(classification_report(test_true, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d9785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv_TextChat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
