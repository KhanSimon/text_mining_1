{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ba93fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
      "  Downloading SQLAlchemy-1.4.54-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "                                              0.0/1.6 MB ? eta -:--:--\n",
      "     ----------                               0.4/1.6 MB 8.9 MB/s eta 0:00:01\n",
      "     --------------------------------         1.3/1.6 MB 13.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 14.5 MB/s eta 0:00:00\n",
      "Collecting alembic>=0.6.2 (from dataset)\n",
      "  Downloading alembic-1.18.3-py3-none-any.whl (262 kB)\n",
      "                                              0.0/262.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 262.3/262.3 kB ? eta 0:00:00\n",
      "Collecting banal>=1.0.1 (from dataset)\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Collecting Mako (from alembic>=0.6.2->dataset)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from alembic>=0.6.2->dataset) (4.15.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy<2.0.0,>=1.3.2->dataset)\n",
      "  Downloading greenlet-3.3.1-cp311-cp311-win_amd64.whl (226 kB)\n",
      "                                              0.0/226.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 226.5/226.5 kB 13.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from Mako->alembic>=0.6.2->dataset) (3.0.3)\n",
      "Installing collected packages: banal, Mako, greenlet, sqlalchemy, alembic, dataset\n",
      "Successfully installed Mako-1.3.10 alembic-1.18.3 banal-1.0.6 dataset-1.6.2 greenlet-3.3.1 sqlalchemy-1.4.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "                                              0.0/515.2 kB ? eta -:--:--\n",
      "     ------------------------------------  512.0/515.2 kB 16.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 515.2/515.2 kB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-23.0.0-cp311-cp311-win_amd64.whl (27.5 MB)\n",
      "                                              0.0/27.5 MB ? eta -:--:--\n",
      "     -                                        1.3/27.5 MB 42.6 MB/s eta 0:00:01\n",
      "     ---                                      2.6/27.5 MB 33.3 MB/s eta 0:00:01\n",
      "     -----                                    3.8/27.5 MB 30.3 MB/s eta 0:00:01\n",
      "     -------                                  4.9/27.5 MB 28.6 MB/s eta 0:00:01\n",
      "     ---------                                6.3/27.5 MB 28.8 MB/s eta 0:00:01\n",
      "     -----------                              8.0/27.5 MB 30.3 MB/s eta 0:00:01\n",
      "     ------------                             8.9/27.5 MB 28.6 MB/s eta 0:00:01\n",
      "     --------------                          10.5/27.5 MB 28.5 MB/s eta 0:00:01\n",
      "     -----------------                       12.5/27.5 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------                     13.9/27.5 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------                  15.6/27.5 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------                17.5/27.5 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------             19.1/27.5 MB 36.4 MB/s eta 0:00:01\n",
      "     -----------------------------           20.7/27.5 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------         22.1/27.5 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------        22.9/27.5 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------      24.1/27.5 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------      24.3/27.5 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------    25.5/27.5 MB 27.3 MB/s eta 0:00:01\n",
      "     -------------------------------------   26.5/27.5 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  27.5/27.5 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  27.5/27.5 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  27.5/27.5 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 27.5/27.5 MB 17.7 MB/s eta 0:00:00\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp311-cp311-win_amd64.whl (31 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Using cached multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Collecting fsspec[http]<=2025.10.0,>=2023.1.0 (from datasets)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.3-cp311-cp311-win_amd64.whl (457 kB)\n",
      "                                              0.0/457.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 457.7/457.7 kB 14.0 MB/s eta 0:00:00\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "                                              0.0/113.6 kB ? eta -:--:--\n",
      "     ---------------------------------------  112.6/113.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 113.6/113.6 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "                                              0.0/44.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.1/44.1 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.1-cp311-cp311-win_amd64.whl (45 kB)\n",
      "                                              0.0/46.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "                                              0.0/41.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.6/41.6 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "                                              0.0/86.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 86.9/86.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tyler marino\\onedrive\\desktop\\gradschool\\saclay\\ai-courses\\textmining-chatbots\\venv_textchat\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, h11, fsspec, frozenlist, dill, attrs, anyio, aiohappyeyeballs, yarl, multiprocess, httpcore, aiosignal, httpx, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2026.1.0\n",
      "    Uninstalling fsspec-2026.1.0:\n",
      "      Successfully uninstalled fsspec-2026.1.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 anyio-4.12.1 attrs-25.4.0 datasets-4.5.0 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 multidict-6.7.1 multiprocess-0.70.18 propcache-0.4.1 pyarrow-23.0.0 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8e9b9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# from gensim.models import Word2Vec\n",
    "# from seqeval.metrics import classification_report, f1_score\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, classification_report\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, EvalPrediction, AutoModelForTokenClassification, Trainer\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import ast\n",
    "# from datasets import load_dataset, ClassLabel, Value, Sequence, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datasets import load_dataset, ClassLabel, Value, Sequence, load_from_disk, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee203ae",
   "metadata": {},
   "source": [
    "### `Transformer Classification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9464fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 128\n",
    "# EMBEDDING_DIM = 100\n",
    "# BATCH_SIZE = 32\n",
    "# EPOCHS = 100\n",
    "# LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# bert_model = args.model\n",
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "bert_model = \"camembert-base\"\n",
    "nb_labels = 2\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65a0109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = \"w2v_med_cbow.model\"\n",
    "\n",
    "TRAIN_FILES = [\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtrain_layer1_ID.csv\",\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/EMEA/EMEAtrain_layer1_ID.csv\",\n",
    "]\n",
    "VALID_FILES = [\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEdev_layer1_ID.csv\",\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/EMEA/EMEAdev_layer1_ID.csv\",\n",
    "]\n",
    "TEST_FILES = [\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/MEDLINE/MEDLINEtest_layer1_ID.csv\",\n",
    "    \"TP_ISD2020/QUAERO_FrenchMed/EMEA/EMEAtest_layer1_ID.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ad480860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LOADING DATA ---\n",
      "Loading MEDLINEtrain_layer1_ID.csv... -> 116 sentences.\n",
      "Loading EMEAtrain_layer1_ID.csv... -> 154 sentences.\n",
      "Loading MEDLINEdev_layer1_ID.csv... -> 115 sentences.\n",
      "Loading EMEAdev_layer1_ID.csv... -> 136 sentences.\n",
      "Loading MEDLINEtest_layer1_ID.csv... -> 120 sentences.\n",
      "Loading EMEAtest_layer1_ID.csv... -> 124 sentences.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. ROBUST DATA LOADER ---\n",
    "def load_data_from_csv(file_paths):\n",
    "    all_sents, all_tags = [], []\n",
    "    for fpath in file_paths:\n",
    "        if not os.path.exists(fpath):\n",
    "            continue\n",
    "        print(f\"Loading {os.path.basename(fpath)}...\", end=\" \")\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                fpath,\n",
    "                sep=None,\n",
    "                engine=\"python\",\n",
    "                keep_default_na=False,\n",
    "                skip_blank_lines=False,\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if \"Mot\" in df.columns:\n",
    "            words, tags = df[\"Mot\"].astype(str).values, df[\"Tag\"].astype(str).values\n",
    "        else:\n",
    "            words, tags = (\n",
    "                df.iloc[:, 0].astype(str).values,\n",
    "                df.iloc[:, -1].astype(str).values,\n",
    "            )\n",
    "\n",
    "        curr_s, curr_t = [], []\n",
    "        file_s, file_t = [], []\n",
    "\n",
    "        for w, t in zip(words, tags):\n",
    "            if not w.strip():\n",
    "                if curr_s:\n",
    "                    file_s.append(curr_s)\n",
    "                    file_t.append(curr_t)\n",
    "                    curr_s, curr_t = [], []\n",
    "            else:\n",
    "                curr_s.append(w)\n",
    "                curr_t.append(t)\n",
    "        if curr_s:\n",
    "            file_s.append(curr_s)\n",
    "            file_t.append(curr_t)\n",
    "\n",
    "        # Chunking Fallback\n",
    "        if len(file_s) < 10 and len(words) > 500:\n",
    "            flat_w = [w for s in file_s for w in s]\n",
    "            flat_t = [t for s in file_t for t in s]\n",
    "            file_s = [flat_w[i : i + MAX_LEN] for i in range(0, len(flat_w), MAX_LEN)]\n",
    "            file_t = [flat_t[i : i + MAX_LEN] for i in range(0, len(flat_t), MAX_LEN)]\n",
    "\n",
    "        print(f\"-> {len(file_s)} sentences.\")\n",
    "        all_sents.extend(file_s)\n",
    "        all_tags.extend(file_t)\n",
    "    return all_sents, all_tags\n",
    "\n",
    "\n",
    "print(\"--- LOADING DATA ---\")\n",
    "train_sents, train_tags = load_data_from_csv(TRAIN_FILES)\n",
    "valid_sents, valid_tags = load_data_from_csv(VALID_FILES)\n",
    "test_sents, test_tags = load_data_from_csv(TEST_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "687b3753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tyler Marino\\OneDrive\\Desktop\\GradSchool\\Saclay\\AI-Courses\\TextMining-ChatBots\\Venv_TextChat\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Tyler Marino\\.cache\\huggingface\\hub\\models--almanach--camembert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 1916.38 examples/s]\n",
      "Map: 100%|██████████| 251/251 [00:00<00:00, 2948.42 examples/s]\n",
      "Map: 100%|██████████| 244/244 [00:00<00:00, 2848.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 3. TOKENIZATION ---\n",
    "unique_tags = sorted(list(set(t for s in train_tags + test_tags for t in s)))\n",
    "label2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "id2label = {i: tag for i, tag in enumerate(unique_tags)}\n",
    "\n",
    "train_ds = Dataset.from_dict({\"tokens\": train_sents, \"ner_tags\": train_tags})\n",
    "valid_ds = Dataset.from_dict({\"tokens\": valid_sents, \"ner_tags\": valid_tags})\n",
    "test_ds = Dataset.from_dict({\"tokens\": test_sents, \"ner_tags\": test_tags})\n",
    "\n",
    "# Try loading Fast Tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "except:\n",
    "    MODEL_NAME = \"almanach/camembert-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_train = train_ds.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_valid = valid_ds.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test = test_ds.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    precision_weighted_average = precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    recall_average = recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    f1_weighted_average = 2.0 * ((precision_weighted_average * recall_average) / ( precision_weighted_average + recall_average ))\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(classification_report(y_pred, y_true))\n",
    "    # return as dictionary\n",
    "    metrics = {'precision weighted': precision_weighted_average,\n",
    "               'recall weighted': recall_average,\n",
    "               'f1 weighted': f1_weighted_average,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def multi_label_metrics_v2(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    return (classification_report(y_true, y_pred))\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metrics(predictions=preds,labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "def padding(examples):\n",
    "    for key in ['input_ids', 'attention_mask']:\n",
    "        for i, element in enumerate(examples[key]):\n",
    "            if len(element) < 20:\n",
    "                for i in range(20 - len(element)):\n",
    "                    element.append(0)\n",
    "            else:\n",
    "                examples[key][i] = element[:20]\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d60b213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 270/270 [00:00<00:00, 2134.05 examples/s]\n",
      "Map: 100%|██████████| 251/251 [00:00<00:00, 1915.49 examples/s]\n",
      "Map: 100%|██████████| 244/244 [00:00<00:00, 2248.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 5. TOKENIZATION & ALIGNMENT ---\n",
    "# BERT breaks words into pieces (\"Hepatitis\" -> \"Hepa\", \"##titis\").\n",
    "# We must align tags so \"Hepa\" gets the label and \"##titis\" is ignored (-100).\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens (CLS, SEP)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])  # Label the first sub-token\n",
    "            else:\n",
    "                label_ids.append(-100)  # Ignore subsequent sub-tokens\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)   # ignored in loss\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)   # subword continuation\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_train = train_ds.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_valid = valid_ds.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_test = test_ds.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "388e0283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight True\n",
      "roberta.embeddings.position_embeddings.weight True\n",
      "roberta.embeddings.token_type_embeddings.weight True\n",
      "roberta.embeddings.LayerNorm.weight True\n",
      "roberta.embeddings.LayerNorm.bias True\n",
      "roberta.encoder.layer.0.attention.self.query.weight True\n",
      "roberta.encoder.layer.0.attention.self.query.bias True\n",
      "roberta.encoder.layer.0.attention.self.key.weight True\n",
      "roberta.encoder.layer.0.attention.self.key.bias True\n",
      "roberta.encoder.layer.0.attention.self.value.weight True\n",
      "roberta.encoder.layer.0.attention.self.value.bias True\n",
      "roberta.encoder.layer.0.attention.output.dense.weight True\n",
      "roberta.encoder.layer.0.attention.output.dense.bias True\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.0.intermediate.dense.weight True\n",
      "roberta.encoder.layer.0.intermediate.dense.bias True\n",
      "roberta.encoder.layer.0.output.dense.weight True\n",
      "roberta.encoder.layer.0.output.dense.bias True\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.1.attention.self.query.weight True\n",
      "roberta.encoder.layer.1.attention.self.query.bias True\n",
      "roberta.encoder.layer.1.attention.self.key.weight True\n",
      "roberta.encoder.layer.1.attention.self.key.bias True\n",
      "roberta.encoder.layer.1.attention.self.value.weight True\n",
      "roberta.encoder.layer.1.attention.self.value.bias True\n",
      "roberta.encoder.layer.1.attention.output.dense.weight True\n",
      "roberta.encoder.layer.1.attention.output.dense.bias True\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.1.intermediate.dense.weight True\n",
      "roberta.encoder.layer.1.intermediate.dense.bias True\n",
      "roberta.encoder.layer.1.output.dense.weight True\n",
      "roberta.encoder.layer.1.output.dense.bias True\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.2.attention.self.query.weight True\n",
      "roberta.encoder.layer.2.attention.self.query.bias True\n",
      "roberta.encoder.layer.2.attention.self.key.weight True\n",
      "roberta.encoder.layer.2.attention.self.key.bias True\n",
      "roberta.encoder.layer.2.attention.self.value.weight True\n",
      "roberta.encoder.layer.2.attention.self.value.bias True\n",
      "roberta.encoder.layer.2.attention.output.dense.weight True\n",
      "roberta.encoder.layer.2.attention.output.dense.bias True\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.2.intermediate.dense.weight True\n",
      "roberta.encoder.layer.2.intermediate.dense.bias True\n",
      "roberta.encoder.layer.2.output.dense.weight True\n",
      "roberta.encoder.layer.2.output.dense.bias True\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.3.attention.self.query.weight True\n",
      "roberta.encoder.layer.3.attention.self.query.bias True\n",
      "roberta.encoder.layer.3.attention.self.key.weight True\n",
      "roberta.encoder.layer.3.attention.self.key.bias True\n",
      "roberta.encoder.layer.3.attention.self.value.weight True\n",
      "roberta.encoder.layer.3.attention.self.value.bias True\n",
      "roberta.encoder.layer.3.attention.output.dense.weight True\n",
      "roberta.encoder.layer.3.attention.output.dense.bias True\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.3.intermediate.dense.weight True\n",
      "roberta.encoder.layer.3.intermediate.dense.bias True\n",
      "roberta.encoder.layer.3.output.dense.weight True\n",
      "roberta.encoder.layer.3.output.dense.bias True\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.4.attention.self.query.weight True\n",
      "roberta.encoder.layer.4.attention.self.query.bias True\n",
      "roberta.encoder.layer.4.attention.self.key.weight True\n",
      "roberta.encoder.layer.4.attention.self.key.bias True\n",
      "roberta.encoder.layer.4.attention.self.value.weight True\n",
      "roberta.encoder.layer.4.attention.self.value.bias True\n",
      "roberta.encoder.layer.4.attention.output.dense.weight True\n",
      "roberta.encoder.layer.4.attention.output.dense.bias True\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.4.intermediate.dense.weight True\n",
      "roberta.encoder.layer.4.intermediate.dense.bias True\n",
      "roberta.encoder.layer.4.output.dense.weight True\n",
      "roberta.encoder.layer.4.output.dense.bias True\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.5.attention.self.query.weight True\n",
      "roberta.encoder.layer.5.attention.self.query.bias True\n",
      "roberta.encoder.layer.5.attention.self.key.weight True\n",
      "roberta.encoder.layer.5.attention.self.key.bias True\n",
      "roberta.encoder.layer.5.attention.self.value.weight True\n",
      "roberta.encoder.layer.5.attention.self.value.bias True\n",
      "roberta.encoder.layer.5.attention.output.dense.weight True\n",
      "roberta.encoder.layer.5.attention.output.dense.bias True\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.5.intermediate.dense.weight True\n",
      "roberta.encoder.layer.5.intermediate.dense.bias True\n",
      "roberta.encoder.layer.5.output.dense.weight True\n",
      "roberta.encoder.layer.5.output.dense.bias True\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.6.attention.self.query.weight True\n",
      "roberta.encoder.layer.6.attention.self.query.bias True\n",
      "roberta.encoder.layer.6.attention.self.key.weight True\n",
      "roberta.encoder.layer.6.attention.self.key.bias True\n",
      "roberta.encoder.layer.6.attention.self.value.weight True\n",
      "roberta.encoder.layer.6.attention.self.value.bias True\n",
      "roberta.encoder.layer.6.attention.output.dense.weight True\n",
      "roberta.encoder.layer.6.attention.output.dense.bias True\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.6.intermediate.dense.weight True\n",
      "roberta.encoder.layer.6.intermediate.dense.bias True\n",
      "roberta.encoder.layer.6.output.dense.weight True\n",
      "roberta.encoder.layer.6.output.dense.bias True\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.7.attention.self.query.weight True\n",
      "roberta.encoder.layer.7.attention.self.query.bias True\n",
      "roberta.encoder.layer.7.attention.self.key.weight True\n",
      "roberta.encoder.layer.7.attention.self.key.bias True\n",
      "roberta.encoder.layer.7.attention.self.value.weight True\n",
      "roberta.encoder.layer.7.attention.self.value.bias True\n",
      "roberta.encoder.layer.7.attention.output.dense.weight True\n",
      "roberta.encoder.layer.7.attention.output.dense.bias True\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.7.intermediate.dense.weight True\n",
      "roberta.encoder.layer.7.intermediate.dense.bias True\n",
      "roberta.encoder.layer.7.output.dense.weight True\n",
      "roberta.encoder.layer.7.output.dense.bias True\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.8.attention.self.query.weight True\n",
      "roberta.encoder.layer.8.attention.self.query.bias True\n",
      "roberta.encoder.layer.8.attention.self.key.weight True\n",
      "roberta.encoder.layer.8.attention.self.key.bias True\n",
      "roberta.encoder.layer.8.attention.self.value.weight True\n",
      "roberta.encoder.layer.8.attention.self.value.bias True\n",
      "roberta.encoder.layer.8.attention.output.dense.weight True\n",
      "roberta.encoder.layer.8.attention.output.dense.bias True\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.8.intermediate.dense.weight True\n",
      "roberta.encoder.layer.8.intermediate.dense.bias True\n",
      "roberta.encoder.layer.8.output.dense.weight True\n",
      "roberta.encoder.layer.8.output.dense.bias True\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.9.attention.self.query.weight True\n",
      "roberta.encoder.layer.9.attention.self.query.bias True\n",
      "roberta.encoder.layer.9.attention.self.key.weight True\n",
      "roberta.encoder.layer.9.attention.self.key.bias True\n",
      "roberta.encoder.layer.9.attention.self.value.weight True\n",
      "roberta.encoder.layer.9.attention.self.value.bias True\n",
      "roberta.encoder.layer.9.attention.output.dense.weight True\n",
      "roberta.encoder.layer.9.attention.output.dense.bias True\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.9.intermediate.dense.weight True\n",
      "roberta.encoder.layer.9.intermediate.dense.bias True\n",
      "roberta.encoder.layer.9.output.dense.weight True\n",
      "roberta.encoder.layer.9.output.dense.bias True\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.10.attention.self.query.weight True\n",
      "roberta.encoder.layer.10.attention.self.query.bias True\n",
      "roberta.encoder.layer.10.attention.self.key.weight True\n",
      "roberta.encoder.layer.10.attention.self.key.bias True\n",
      "roberta.encoder.layer.10.attention.self.value.weight True\n",
      "roberta.encoder.layer.10.attention.self.value.bias True\n",
      "roberta.encoder.layer.10.attention.output.dense.weight True\n",
      "roberta.encoder.layer.10.attention.output.dense.bias True\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.10.intermediate.dense.weight True\n",
      "roberta.encoder.layer.10.intermediate.dense.bias True\n",
      "roberta.encoder.layer.10.output.dense.weight True\n",
      "roberta.encoder.layer.10.output.dense.bias True\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.11.attention.self.query.weight True\n",
      "roberta.encoder.layer.11.attention.self.query.bias True\n",
      "roberta.encoder.layer.11.attention.self.key.weight True\n",
      "roberta.encoder.layer.11.attention.self.key.bias True\n",
      "roberta.encoder.layer.11.attention.self.value.weight True\n",
      "roberta.encoder.layer.11.attention.self.value.bias True\n",
      "roberta.encoder.layer.11.attention.output.dense.weight True\n",
      "roberta.encoder.layer.11.attention.output.dense.bias True\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "roberta.encoder.layer.11.intermediate.dense.weight True\n",
      "roberta.encoder.layer.11.intermediate.dense.bias True\n",
      "roberta.encoder.layer.11.output.dense.weight True\n",
      "roberta.encoder.layer.11.output.dense.bias True\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight True\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias True\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(bert_model, num_labels=nb_labels)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"finetune_\" + bert_model.replace(\"/\",\"_\") + \"_\" + str(EPOCHS) + \"_\" + str(TRAIN_BATCH_SIZE),\n",
    "    save_strategy = \"epoch\",\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    load_best_model_at_end=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "28085613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyler Marino\\AppData\\Local\\Temp\\ipykernel_41712\\440837247.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_valid, tokenizer=tokenizer, compute_metrics=compute_metrics)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_valid, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(\"************** RESULTS for \" + bert_model.replace(\"/\",\"_\") + \" *****************\")\n",
    "print(\"Training time: \" + str(elapsed) + \" ms\")\n",
    "results_valid = trainer.evaluate(tokenized_valid)\n",
    "print(results_valid)\n",
    "results_test = trainer.evaluate(tokenized_test)\n",
    "print(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f03cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST RESULTS ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ANAT       0.38      0.24      0.30       364\n",
      "        CHEM       0.45      0.19      0.27      1037\n",
      "        DEVI       0.15      0.04      0.06       107\n",
      "        DISO       0.35      0.24      0.29       977\n",
      "        GEOG       0.50      0.05      0.09        63\n",
      "        LIVB       0.75      0.57      0.65       498\n",
      "        OBJC       0.14      0.04      0.06        81\n",
      "        PHEN       0.00      0.00      0.00        70\n",
      "        PHYS       0.48      0.21      0.29       190\n",
      "        PROC       0.41      0.49      0.45       761\n",
      "\n",
      "   micro avg       0.44      0.30      0.35      4148\n",
      "   macro avg       0.36      0.21      0.24      4148\n",
      "weighted avg       0.43      0.30      0.34      4148\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idrissamahamoudoudicko/anaconda3/envs/HandonNLP/lib/python3.13/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# --- 7. FINAL TEST EVALUATION ---\n",
    "print(\"\\n--- TEST RESULTS ---\")\n",
    "model.load_state_dict(torch.load(\"best_cnn_ner.pt\"))\n",
    "model.eval()\n",
    "test_true, test_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        out = model(x)\n",
    "        preds = torch.argmax(out, dim=2).cpu().numpy()\n",
    "        labels = y.numpy()\n",
    "        for i in range(len(x)):\n",
    "            p_s, t_s = [], []\n",
    "            for j in range(SEQUENCE_LENGTH):\n",
    "                if labels[i][j] == 0:\n",
    "                    break\n",
    "                p_s.append(idx2tag[preds[i][j]])\n",
    "                t_s.append(idx2tag[labels[i][j]])\n",
    "            test_pred.append(p_s)\n",
    "            test_true.append(t_s)\n",
    "\n",
    "print(classification_report(test_true, test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv_TextChat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
