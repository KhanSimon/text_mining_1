{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Idrissa Dicko & Tyler Marino & Simon Khan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm\n",
    "! python -m spacy download fr_core_news_sm\n",
    "#! pip install nltk pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 : Lemmatization\n",
    "\n",
    "In this exercise, the objective is to create your own lemmatizer for french language. We will test different lemmatization approaches : \n",
    "* Based on a dictionary\n",
    "* Based on machine learning approach (you can use sklearn) or define your own architecture with pytorch\n",
    "* With and without pos tag given as input\n",
    "\n",
    "In all case you should compare your results and report performances of the proposed algorithm to [spacy](https://spacy.io/models/fr) lemmatizer (the different configuration).\n",
    "\n",
    "You are free to use any machine-learning algorihtm/model, taking or not the context of sentences such as [LinearRegression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html) or training your own [RNN with pytorch](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). \n",
    "However you must always motivate your choices and compare results of the different configurations.\n",
    "\n",
    "You will send the report to *thomas.gerald@universite-paris-saclay.fr* in PDF format named as following and the code (notebook with  output of the two exercises in a zip format) :\n",
    "\n",
    "\n",
    "**report_[firstname]_[lastname].pdf**\n",
    "\n",
    "The report for the two exercises must not exceed three pages !\n",
    "\n",
    "\n",
    "## Dataset\n",
    "To train or build your lemmatizer you have three files in *tabular separated values* format :\n",
    "* [training-set.tsv](https://thomas-gerald.fr/TMC/resources/data/training-set.tsv) that you can use to train/build your dictionnary/model \n",
    "* [testing-set.tsv](https://thomas-gerald.fr/TMC/resources/data/testing-set.tsv) used to evaluate the different approaches\n",
    "* [testing-gallica.tsv](https://thomas-gerald.fr/TMC/resources/data/testing-gallica.tsv) used as gold standard to evaluate performances [github (in french)](https://github.com/Gallicorpora/Lemmatisation)\n",
    "\n",
    "In our case we have two possibilities for a lemma:\n",
    "* (a) A sequence of characters, meaning that \"to rule\" an \"a rule\" are the same lemma\n",
    "* (b) A sequence of characters, meaning that \"to rule\" represent the verb, a tuple (\"rule\", \"V\") while \"a rule\" is represented by the tuple (\"rule\", \"N\") \n",
    "In the (a) case the size of the vocabulary (output) will be \n",
    "## Spacy :\n",
    "\n",
    "Below a small example using spacy lemmatization\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_a = \"He is thirty years old\"\n",
    "text_b = \"We still are champions\"\n",
    "print(f'Lemmatization A : {[(w.lemma_, w.pos_) for w in nlp(text_a)]}')\n",
    "print(f'Lemmatization B : {[(w.lemma_, w.pos_) for w in nlp(text_b)]}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation A: [('on', 'PRON'), ('être', 'AUX'), ('toujours', 'ADV'), ('champion', 'NOUN')]\n",
      "Lemmatisation B: [('il', 'PRON'), ('avoir', 'AUX'), ('trente', 'NUM'), ('an', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "text_a = \"On est toujours champions\"\n",
    "text_b=\"Il a trente ans\"\n",
    "print(f\"Lemmatisation A: {[(w.lemma_,w.pos_)for w in nlp_fr(text_a)]}\")\n",
    "print(f\"Lemmatisation B: {[(w.lemma_,w.pos_)for w in nlp_fr(text_b)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data\n",
    "\n",
    "You can use pandas to read the data using tabular separator as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Setup ---\n",
      "✔ spaCy model 'fr_core_news_sm' loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. SETUP & CONFIGURATION ---\n",
    "print(\"--- 1. Setup ---\")\n",
    "# Download/Load spaCy model\n",
    "try:\n",
    "    nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "    print(\"✔ spaCy model 'fr_core_news_sm' loaded.\")\n",
    "except OSError:\n",
    "    print(\"✘ spaCy model not found. Downloading...\")\n",
    "    os.system(\"python -m spacy download fr_core_news_sm\")\n",
    "    nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Define File Paths\n",
    "TRAIN_PATH = \"training-set.tsv\"\n",
    "TEST_PATH = \"testing-set.tsv\"\n",
    "GALLICA_PATH = \"testing-gallica.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DATA LOADING FUNCTIONS ---\n",
    "\n",
    "def read_standard_tsv(path):\n",
    "    \"\"\"Reads the standard training/testing files (No header, simple tags).\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"⚠ File not found: {path}\")\n",
    "        return pd.DataFrame(columns=[\"token\", \"lemma\", \"pos\"])\n",
    "\n",
    "    # Read (No header in these files)\n",
    "    df = pd.read_csv(path, sep=\"\\t\", names=[\n",
    "                     \"token\", \"lemma\", \"pos\"], keep_default_na=False, quoting=3)\n",
    "\n",
    "    # Normalize\n",
    "    df[\"pos\"] = df[\"pos\"].replace(\"\", \"X\")\n",
    "    df[\"token\"] = df[\"token\"].astype(str)\n",
    "    df[\"lemma\"] = df[\"lemma\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_gallica_tsv(path):\n",
    "    \"\"\"Reads the Gallica file (Has header, complex tags).\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"⚠ File not found: {path}\")\n",
    "        return pd.DataFrame(columns=[\"token\", \"lemma\", \"pos\"])\n",
    "\n",
    "    # Read (Has header: form, lemma, POS, morph)\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=0,\n",
    "                     quoting=3, keep_default_na=False)\n",
    "\n",
    "    # Rename columns to match our standard\n",
    "    # Based on screenshot: 'form' -> token, 'POS' -> pos\n",
    "    rename_map = {'form': 'token', 'POS': 'pos'}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Ensure columns exist\n",
    "    if 'token' not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return df[['token', 'lemma', 'pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TRAINING FUNCTIONS (DICTIONARY MODELS) ---\n",
    "\n",
    "def train_model_a(df):\n",
    "    \"\"\"Model A: Token -> Most Frequent Lemma\"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    for tok, lem in zip(df[\"token\"], df[\"lemma\"]):\n",
    "        counts[tok][lem] += 1\n",
    "\n",
    "    model = {}\n",
    "    for tok, c in counts.items():\n",
    "        model[tok] = c.most_common(1)[0][0]\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model_b(df):\n",
    "    \"\"\"Model B: (Token, POS) -> Most Frequent Lemma\"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    for tok, pos, lem in zip(df[\"token\"], df[\"pos\"], df[\"lemma\"]):\n",
    "        counts[(tok, pos)][lem] += 1\n",
    "\n",
    "    model = {}\n",
    "    for key, c in counts.items():\n",
    "        model[key] = c.most_common(1)[0][0]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. PREDICTION & MAPPING FUNCTIONS ---\n",
    "\n",
    "def spacy_lemma(token):\n",
    "    \"\"\"Baseline: Uses spaCy for lemmatization.\"\"\"\n",
    "    if nlp_fr is None:\n",
    "        return token\n",
    "    doc = nlp_fr(token)\n",
    "    return doc[0].lemma_ if len(doc) else \"\"\n",
    "\n",
    "\n",
    "def predict_model_a(model, tokens, fallback=\"spacy\"):\n",
    "    preds = []\n",
    "    for t in tokens:\n",
    "        if t in model:\n",
    "            preds.append(model[t])\n",
    "        else:\n",
    "            if fallback == \"spacy\":\n",
    "                preds.append(spacy_lemma(t))\n",
    "            elif fallback == \"lower\":\n",
    "                preds.append(t.lower())\n",
    "            else:\n",
    "                preds.append(t)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def predict_model_b(model, tokens, poses, fallback=\"spacy\"):\n",
    "    preds = []\n",
    "    for t, p in zip(tokens, poses):\n",
    "        key = (t, p)\n",
    "        if key in model:\n",
    "            preds.append(model[key])\n",
    "        else:\n",
    "            if fallback == \"spacy\":\n",
    "                preds.append(spacy_lemma(t))\n",
    "            elif fallback == \"lower\":\n",
    "                preds.append(t.lower())\n",
    "            else:\n",
    "                preds.append(t)\n",
    "    return preds\n",
    "\n",
    "\n",
    "# Map Gallica tags (VERcjg) to Training tags (V)\n",
    "gallica_map = {\n",
    "    'VER': 'V', 'NOM': 'N', 'ADJ': 'A', 'ADV': 'ADV',\n",
    "    'DET': 'D', 'CON': 'C', 'PRO': 'PRO', 'PRE': 'P', 'PON': 'PONCT'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_gallica_pos(tag):\n",
    "    prefix = str(tag)[:3]  # Take first 3 chars\n",
    "    return gallica_map.get(prefix, tag)  # Map or keep original\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred, label=\"\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"  > {label:<35} Accuracy: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Loading Data ---\n",
      "  Training Set: 261389 rows\n",
      "  Testing Set:  16694 rows\n",
      "  Gallica Set:  2841 rows\n"
     ]
    }
   ],
   "source": [
    "# --- 5. MAIN EXECUTION ---\n",
    "\n",
    "# A. LOAD DATA\n",
    "print(\"\\n--- 2. Loading Data ---\")\n",
    "full_train_df = read_standard_tsv(TRAIN_PATH)\n",
    "test_df = read_standard_tsv(TEST_PATH)\n",
    "gallica_df = read_gallica_tsv(GALLICA_PATH)\n",
    "\n",
    "print(f\"  Training Set: {len(full_train_df)} rows\")\n",
    "print(f\"  Testing Set:  {len(test_df)} rows\")\n",
    "print(f\"  Gallica Set:  {len(gallica_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Validation: Tuning Fallback Strategy ---\n",
      "  Strategy 'spacy': 0.9677\n",
      "  Strategy 'lower': 0.9519\n",
      "  Strategy 'identity': 0.9575\n",
      "✔ Selected Best Strategy: 'spacy'\n",
      "\n",
      "--- 4. Training Final Models ---\n",
      "✔ Models trained on full dataset.\n",
      "\n",
      "--- 5. Results: Standard Test Set ---\n",
      "  > MFL (Token only)                    Accuracy: 0.9531\n",
      "  > MFL (Token + POS)                   Accuracy: 0.9653\n",
      "  > SpaCy Baseline                      Accuracy: 0.8340\n",
      "\n",
      "--- 6. Results: Gallica (Historical) ---\n",
      "  > MFL token → lemma                   Accuracy: 0.4812\n",
      "  > MFL (token, PoS) → lemma            Accuracy: 0.4917\n",
      "  > SpaCy Baseline                      Accuracy: 0.4910\n",
      "\n",
      "--- Done ---\n"
     ]
    }
   ],
   "source": [
    "# B. VALIDATION STEP (Tuning Fallback)\n",
    "print(\"\\n--- 3. Validation: Tuning Fallback Strategy ---\")\n",
    "train_split, val_split = train_test_split(\n",
    "    full_train_df, test_size=0.1, random_state=42)\n",
    "temp_model = train_model_b(train_split)\n",
    "\n",
    "best_acc = 0\n",
    "best_strat = \"identity\"\n",
    "for strat in [\"spacy\", \"lower\", \"identity\"]:\n",
    "    preds = predict_model_b(\n",
    "        temp_model, val_split[\"token\"], val_split[\"pos\"], fallback=strat)\n",
    "    acc = accuracy_score(val_split[\"lemma\"], preds)\n",
    "    print(f\"  Strategy '{strat}': {acc:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_strat = strat\n",
    "\n",
    "print(f\"✔ Selected Best Strategy: '{best_strat}'\")\n",
    "\n",
    "# C. FINAL TRAINING\n",
    "print(\"\\n--- 4. Training Final Models ---\")\n",
    "model_a = train_model_a(full_train_df)\n",
    "model_b = train_model_b(full_train_df)\n",
    "print(\"✔ Models trained on full dataset.\")\n",
    "\n",
    "# D. EVALUATION: STANDARD TEST SET\n",
    "print(\"\\n--- 5. Results: Standard Test Set ---\")\n",
    "# Model A\n",
    "preds_a = predict_model_a(model_a, test_df[\"token\"], fallback=best_strat)\n",
    "evaluate(test_df[\"lemma\"], preds_a, label=\"MFL (Token only)\")\n",
    "\n",
    "# Model B\n",
    "preds_b = predict_model_b(\n",
    "    model_b, test_df[\"token\"], test_df[\"pos\"], fallback=best_strat)\n",
    "evaluate(test_df[\"lemma\"], preds_b, label=\"MFL (Token + POS)\")\n",
    "\n",
    "# SpaCy Baseline\n",
    "preds_spacy = [spacy_lemma(t) for t in test_df[\"token\"]]\n",
    "evaluate(test_df[\"lemma\"], preds_spacy, label=\"SpaCy Baseline\")\n",
    "\n",
    "# E. EVALUATION: GALLICA GOLD STANDARD\n",
    "print(\"\\n--- 6. Results: Gallica (Historical) ---\")\n",
    "if len(gallica_df) > 0:\n",
    "    # 1. Evaluate Model A (Unaffected by POS tags)\n",
    "    preds_g_a = predict_model_a(\n",
    "        model_a, gallica_df[\"token\"], fallback=best_strat)\n",
    "    evaluate(gallica_df[\"lemma\"], preds_g_a,\n",
    "             label=\"MFL token → lemma \")\n",
    "\n",
    "    # 2. Evaluate Model B (Needs POS Mapping)\n",
    "    # Map 'VERcjg' -> 'V' so the dictionary keys match\n",
    "    gallica_pos_fixed = gallica_df[\"pos\"].apply(normalize_gallica_pos)\n",
    "    preds_g_b = predict_model_b(\n",
    "        model_b, gallica_df[\"token\"], gallica_pos_fixed, fallback=best_strat)\n",
    "    evaluate(gallica_df[\"lemma\"], preds_g_b,\n",
    "             label=\"MFL (token, PoS) → lemma\")\n",
    "\n",
    "    # 3. SpaCy\n",
    "    preds_g_spacy = [spacy_lemma(t) for t in gallica_df[\"token\"]]\n",
    "    evaluate(gallica_df[\"lemma\"], preds_g_spacy, label=\"SpaCy Baseline\")\n",
    "else:\n",
    "    print(\"⚠ Gallica dataset skipped (empty).\")\n",
    "\n",
    "print(\"\\n--- Done ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Evaluating Standard Test Set ---\n",
      "| Model                    |   Overall Accuracy |   Known Accuracy |   Unknown Accuracy |\n",
      "|:-------------------------|-------------------:|-----------------:|-------------------:|\n",
      "| MFL token → lemma        |             0.9531 |           0.9694 |             0.6988 |\n",
      "| MFL (token, PoS) → lemma |             0.9653 |           0.9824 |             0.6988 |\n",
      "| SpaCy Baseline           |             0.834  |           0.8427 |             0.6988 |\n"
     ]
    }
   ],
   "source": [
    "# --- EVALUATION: STANDARD TEST SET (With Known/Unknown Table) ---\n",
    "print(\"\\n--- 7. Evaluating Standard Test Set ---\")\n",
    "\n",
    "# Reuse the metric helper function\n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_pred, tokens, training_tokens_set):\n",
    "    # 1. Overall Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # 2. Known vs Unknown Mask\n",
    "    is_known = tokens.isin(training_tokens_set)\n",
    "\n",
    "    # 3. Known Accuracy\n",
    "    if is_known.sum() > 0:\n",
    "        acc_known = accuracy_score(\n",
    "            y_true[is_known], pd.Series(y_pred)[is_known])\n",
    "    else:\n",
    "        acc_known = 0.0\n",
    "\n",
    "    # 4. Unknown Accuracy\n",
    "    if (~is_known).sum() > 0:\n",
    "        acc_unk = accuracy_score(\n",
    "            y_true[~is_known], pd.Series(y_pred)[~is_known])\n",
    "    else:\n",
    "        acc_unk = 0.0\n",
    "\n",
    "    return acc, acc_known, acc_unk\n",
    "\n",
    "\n",
    "results_test = []\n",
    "train_tokens_set = set(full_train_df[\"token\"])  # Define known vocabulary\n",
    "\n",
    "# 1. Model A (Token -> Lemma)\n",
    "preds_a = predict_model_a(model_a, test_df[\"token\"], fallback=best_strat)\n",
    "acc, known, unk = get_metrics(\n",
    "    test_df[\"lemma\"], preds_a, test_df[\"token\"], train_tokens_set)\n",
    "results_test.append([\"MFL token → lemma \", acc, known, unk])\n",
    "\n",
    "# 2. Model B (Token + POS -> Lemma)\n",
    "preds_b = predict_model_b(\n",
    "    model_b, test_df[\"token\"], test_df[\"pos\"], fallback=best_strat)\n",
    "acc, known, unk = get_metrics(\n",
    "    test_df[\"lemma\"], preds_b, test_df[\"token\"], train_tokens_set)\n",
    "results_test.append([\" MFL (token, PoS) → lemma\", acc, known, unk])\n",
    "\n",
    "# 3. SpaCy Baseline\n",
    "preds_spacy = [spacy_lemma(t) for t in test_df[\"token\"]]\n",
    "acc, known, unk = get_metrics(\n",
    "    test_df[\"lemma\"], preds_spacy, test_df[\"token\"], train_tokens_set)\n",
    "results_test.append([\"SpaCy Baseline\", acc, known, unk])\n",
    "\n",
    "# --- Display Table ---\n",
    "df_results_test = pd.DataFrame(results_test, columns=[\n",
    "                               \"Model\", \"Overall Accuracy\", \"Known Accuracy\", \"Unknown Accuracy\"])\n",
    "print(df_results_test.round(4).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results — Summary\n",
    "\n",
    "The MFL models (most-frequent-lemma) outperform spaCy on the standard test set. Best model (Token+PoS) — 96.53% vs SpaCy 83.40% (≈13.13 points).\n",
    "\n",
    "---\n",
    "\n",
    "### Standard test set — Known vs Unknown\n",
    "\n",
    "| Model | Known accuracy | Unknown accuracy | Overall |\n",
    "|---|---:|---:|---:|\n",
    "| MFL (Token) | 96.94% | 69.88% | 95.31% |\n",
    "| MFL (Token + PoS) | 98.24% | 69.88% | 96.53% |\n",
    "| SpaCy Baseline | 84.27% | 69.88% | 83.40% |\n",
    "\n",
    "Key points:\n",
    "- Dictionary-based models nearly perfect on known tokens (≥96.9%).\n",
    "- PoS information yields a clear improvement for known tokens (+≈1.3 pp).\n",
    "- Unknown-token performance is identical across models (69.88%) because the selected fallback is spaCy.\n",
    "\n",
    "---\n",
    "\n",
    "### Gallica (historical) evaluation\n",
    "\n",
    "| Model | Known accuracy | Unknown accuracy | Overall |\n",
    "|---|---:|---:|---:|\n",
    "| Model A (Token) | 82.31% | 8.44% | 48.12% |\n",
    "| Model B (Token + POS) | 84.27% | 8.44% | 49.17% |\n",
    "| SpaCy Baseline | 84.14% | 8.44% | 49.10% |\n",
    "\n",
    "Key points:\n",
    "- Severe drop in overall accuracy (~49%) due to massive OOV rate (archaic spellings).\n",
    "- Known-token accuracy remains high (~84%), but unknown-token accuracy collapses (8.44%).\n",
    "\n",
    "---\n",
    "\n",
    "### Short conclusions & next steps\n",
    "- MFL is a very strong baseline when train/test domains match; PoS helps for disambiguation.\n",
    "- Main limitation: out-of-vocabulary generalization — current fallback (spaCy) caps unknown performance.\n",
    "- Remedies to try: subword/character models, augment training with historical variants, or train a neural seq2seq / copy-capable model as fallback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Baseline: Most-Frequent Lemma\n",
    "\n",
    "The simplicity of the most-frequent-lemma approach is a strength when the training and testing domains match. It consistently achieves a high accuracy of 96.9%. However, the main limitation of this baseline is its poor performance on out-of-vocabulary tokens, particularly in domain-shift scenarios where simple dictionary lookups and modern fallbacks are insufficient.\n",
    "\n",
    "### PoS Tags\n",
    "\n",
    "Including PoS tags provides a clear benefit for known words. By leveraging the part-of-speech information, the accuracy improves from 96.9% to 98.2%. This suggests that the additional information about word meaning can significantly improve the tagger's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "We are not using Deep Learning (DL) for this task of lemmatization of French words because:\n",
    "\n",
    "1. **Lexical Determinism**: In standard French, lemmatization is highly deterministic. The vast majority of tokens map to a single lemma. Even ambiguous forms like \"s\", which can be a verb or a noun, can be almost perfectly resolved with a simple Part-of-Speech (POS) tag. This implies that a lookup table (dictionary) can capture these direct mappings instantly. A neural network would have to spend thousands of epochs \"learning\" to memorize these simple pairs, essentially reinventing the dictionary at a much higher computational cost.\n",
    "\n",
    "2. **Domain Matching**: The training and testing datasets come from the same domain and follow the same annotation standards. Deep learning excels at generalization—inferring rules for unseen data distributions (e.g., guessing the lemma of a made-up word based on its suffix). However, in this specific exercise, the test set vocabulary overlaps heavily with the training set (high \"known word\" percentage), the generalization power of a neural network yields diminishing returns. The dictionary model already achieves ~96.5% accuracy, leaving very little room for improvement.\n",
    "\n",
    "3. **Efficiency and Complexity**: Training Time: A dictionary model trains in less than a second on a standard CPU. A sequence-to-sequence RNN requires significant training time, hyperparameter tuning (learning rate, hidden dimensions, layers), and ideally a GPU. Inference Speed: Dictionary lookups are O(1) (constant time). Neural inference involves complex matrix multiplications (O(N^2) or worse depending on architecture), making it orders of magnitude slower for production use without specialized hardware.\n",
    "\n",
    "4. **Handling Unknown Words**: The main theoretical advantage of a character-level neural network is handling Out-of-Vocabulary (OOV) words by learning morphological patterns (e.g., seeing \"-ait\" implies an imperfect verb). However, in this specific exercise, the dictionary model—paired with a simple fallback strategy (like using spaCy for unknown words)—already achieved a competitive baseline. While a custom RNN could potentially outperform the spaCy fallback on unknown words, the engineering effort required to tune it to beat a pre-trained industrial model is often disproportionate to the gain.\n",
    "\n",
    "Therefore, using a dictionary-based approach is a good choice for this specific task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HandonNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
