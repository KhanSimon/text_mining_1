{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Idrissa Dicko & Tyler Marino & Simon Khan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm\n",
    "! python -m spacy download fr_core_news_sm\n",
    "#! pip install nltk pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 : Lemmatization\n",
    "\n",
    "In this exercise, the objective is to create your own lemmatizer for french language. We will test different lemmatization approaches : \n",
    "* Based on a dictionary\n",
    "* Based on machine learning approach (you can use sklearn) or define your own architecture with pytorch\n",
    "* With and without pos tag given as input\n",
    "\n",
    "In all case you should compare your results and report performances of the proposed algorithm to [spacy](https://spacy.io/models/fr) lemmatizer (the different configuration).\n",
    "\n",
    "You are free to use any machine-learning algorihtm/model, taking or not the context of sentences such as [LinearRegression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html) or training your own [RNN with pytorch](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). \n",
    "However you must always motivate your choices and compare results of the different configurations.\n",
    "\n",
    "You will send the report to *thomas.gerald@universite-paris-saclay.fr* in PDF format named as following and the code (notebook with  output of the two exercises in a zip format) :\n",
    "\n",
    "\n",
    "**report_[firstname]_[lastname].pdf**\n",
    "\n",
    "The report for the two exercises must not exceed three pages !\n",
    "\n",
    "\n",
    "## Dataset\n",
    "To train or build your lemmatizer you have three files in *tabular separated values* format :\n",
    "* [training-set.tsv](https://thomas-gerald.fr/TMC/resources/data/training-set.tsv) that you can use to train/build your dictionnary/model \n",
    "* [testing-set.tsv](https://thomas-gerald.fr/TMC/resources/data/testing-set.tsv) used to evaluate the different approaches\n",
    "* [testing-gallica.tsv](https://thomas-gerald.fr/TMC/resources/data/testing-gallica.tsv) used as gold standard to evaluate performances [github (in french)](https://github.com/Gallicorpora/Lemmatisation)\n",
    "\n",
    "In our case we have two possibilities for a lemma:\n",
    "* (a) A sequence of characters, meaning that \"to rule\" an \"a rule\" are the same lemma\n",
    "* (b) A sequence of characters, meaning that \"to rule\" represent the verb, a tuple (\"rule\", \"V\") while \"a rule\" is represented by the tuple (\"rule\", \"N\") \n",
    "In the (a) case the size of the vocabulary (output) will be \n",
    "## Spacy :\n",
    "\n",
    "Below a small example using spacy lemmatization\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_a = \"He is thirty years old\"\n",
    "text_b = \"We still are champions\"\n",
    "print(f'Lemmatization A : {[(w.lemma_, w.pos_) for w in nlp(text_a)]}')\n",
    "print(f'Lemmatization B : {[(w.lemma_, w.pos_) for w in nlp(text_b)]}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation A: [('on', 'PRON'), ('être', 'AUX'), ('toujours', 'ADV'), ('champion', 'NOUN')]\n",
      "Lemmatisation B: [('il', 'PRON'), ('avoir', 'AUX'), ('trente', 'NUM'), ('an', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "text_a = \"On est toujours champions\"\n",
    "text_b=\"Il a trente ans\"\n",
    "print(f\"Lemmatisation A: {[(w.lemma_,w.pos_)for w in nlp_fr(text_a)]}\")\n",
    "print(f\"Lemmatisation B: {[(w.lemma_,w.pos_)for w in nlp_fr(text_b)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data\n",
    "\n",
    "You can use pandas to read the data using tabular separator as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Certes</td>\n",
       "      <td>certes</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PONCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rien</td>\n",
       "      <td>rien</td>\n",
       "      <td>PRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ne</td>\n",
       "      <td>ne</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dit</td>\n",
       "      <td>dire</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261384</th>\n",
       "      <td>effet</td>\n",
       "      <td>effet</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261385</th>\n",
       "      <td>positif</td>\n",
       "      <td>positif</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261386</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PONCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261387</th>\n",
       "      <td>tenir</td>\n",
       "      <td>tenir</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261388</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PONCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261389 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token    lemma    pos\n",
       "0        Certes   certes    ADV\n",
       "1             ,        ,  PONCT\n",
       "2          rien     rien    PRO\n",
       "3            ne       ne    ADV\n",
       "4           dit     dire      V\n",
       "...         ...      ...    ...\n",
       "261384    effet    effet      N\n",
       "261385  positif  positif      A\n",
       "261386        .        .  PONCT\n",
       "261387    tenir    tenir      V\n",
       "261388        .        .  PONCT\n",
       "\n",
       "[261389 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_file = \"data/training-set.tsv\"\n",
    "pd.read_csv(train_file, sep='\\t', names=[\"token\", \"lemma\", \"pos\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input vocabulary contains : 23271 words\n",
      "The number of str lemma is :  15194\n",
      "The number of lemma (considering PoS) is :  16144\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w_vocabulary = {'unknow_word'}\n",
    "l_vocabulary = set()\n",
    "lp_vocabulary = set()\n",
    "\n",
    "with open(train_file, 'r')  as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            word, lemma, pos = line.split()\n",
    "            w_vocabulary.add(word)\n",
    "            l_vocabulary.add(lemma)\n",
    "            lp_vocabulary.add((lemma, pos))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f'The input vocabulary contains : {len(w_vocabulary)} words' )\n",
    "print(f'The number of str lemma is :  {len(l_vocabulary)}')\n",
    "print(f'The number of lemma (considering PoS) is :  {len(lp_vocabulary)}') #marche (le nom) est différent de marche (le verbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy\n",
    "\n",
    "#reading TSV files\n",
    "\n",
    "def read_tsv(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\", names=[\"token\", \"lemma\", \"pos\"], keep_default_na=False)\n",
    "    # normalize empties\n",
    "    df[\"pos\"] = df[\"pos\"].replace(\"\", \"X\")\n",
    "    df[\"token\"] = df[\"token\"].astype(str)\n",
    "    df[\"lemma\"] = df[\"lemma\"].astype(str)\n",
    "    return df\n",
    "\n",
    "train_path = \"data/training-set.tsv\"\n",
    "test_path  = \"data/testing-set.tsv\"\n",
    "\n",
    "train_df = read_tsv(train_path)\n",
    "test_df  = read_tsv(test_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "#model A: token -> most frequent lemma\n",
    "def train_mfl_token(df):\n",
    "    counts = defaultdict(Counter)\n",
    "    for tok, lem in zip(df[\"token\"], df[\"lemma\"]):\n",
    "        counts[tok][lem] += 1\n",
    "    model = {}\n",
    "    for tok, c in counts.items():\n",
    "        model[tok] = c.most_common(1)[0][0]\n",
    "    return model\n",
    "\n",
    "#model B: (token,pos) -> most frequent lemma\n",
    "def train_mfl_token_pos(df):\n",
    "    counts = defaultdict(Counter)\n",
    "    for tok, pos, lem in zip(df[\"token\"], df[\"pos\"], df[\"lemma\"]):\n",
    "        counts[(tok, pos)][lem] += 1\n",
    "    model = {}\n",
    "    for key, c in counts.items():\n",
    "        model[key] = c.most_common(1)[0][0]\n",
    "    return model\n",
    "\n",
    "mfl_tok = train_mfl_token(train_df)\n",
    "mfl_tok_pos = train_mfl_token_pos(train_df)\n",
    "\n",
    "#spacy lemma function (token-by-token, no sentence context here)\n",
    "def spacy_lemma(token):\n",
    "    doc = nlp_fr(token)\n",
    "    return doc[0].lemma_ if len(doc) else \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFL token->lemma (fallback spaCy) accuracy = 0.9531\n",
      "  known accuracy = 0.9694\n",
      "  unk   accuracy = 0.6988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sisso/documents_local/m2_ai/text_mining_and_chatbots/text_mining_1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/Users/sisso/documents_local/m2_ai/text_mining_and_chatbots/text_mining_1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFL (token,pos)->lemma (fallback spaCy) accuracy = 0.9653\n",
      "  known accuracy = 0.9855\n",
      "  unk   accuracy = 0.6828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sisso/documents_local/m2_ai/text_mining_and_chatbots/text_mining_1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/Users/sisso/documents_local/m2_ai/text_mining_and_chatbots/text_mining_1/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy token lemma baseline accuracy = 0.8150\n"
     ]
    }
   ],
   "source": [
    "#INFERENCE\n",
    "# ---- Predict functions (with optional fallback)\n",
    "def predict_mfl_token(tokens, fallback=\"spacy\"):\n",
    "    preds = []\n",
    "    for t in tokens:\n",
    "        if t in mfl_tok:\n",
    "            preds.append(mfl_tok[t])\n",
    "        else:\n",
    "            if fallback == \"spacy\":\n",
    "                preds.append(spacy_lemma(t))\n",
    "            elif fallback == \"lower\":\n",
    "                preds.append(t.lower())\n",
    "            else:\n",
    "                preds.append(t)  # identity\n",
    "    return preds\n",
    "\n",
    "def predict_mfl_token_pos(tokens, poses, fallback=\"spacy\"):\n",
    "    preds = []\n",
    "    for t, p in zip(tokens, poses):\n",
    "        key = (t, p)\n",
    "        if key in mfl_tok_pos:\n",
    "            preds.append(mfl_tok_pos[key])\n",
    "        else:\n",
    "            if fallback == \"spacy\":\n",
    "                preds.append(spacy_lemma(t))\n",
    "            elif fallback == \"lower\":\n",
    "                preds.append(t.lower())\n",
    "            else:\n",
    "                preds.append(t)\n",
    "    return preds\n",
    "\n",
    "def evaluate(y_true, y_pred, known_mask=None, label=\"\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"{label} accuracy = {acc:.4f}\")\n",
    "    if known_mask is not None:\n",
    "        acc_known = accuracy_score(y_true[known_mask], pd.Series(y_pred)[known_mask])\n",
    "        acc_unk   = accuracy_score(y_true[~known_mask], pd.Series(y_pred)[~known_mask])\n",
    "        print(f\"  known accuracy = {acc_known:.4f}\")\n",
    "        print(f\"  unk   accuracy = {acc_unk:.4f}\")\n",
    "\n",
    "y_true = test_df[\"lemma\"]\n",
    "tokens = test_df[\"token\"]\n",
    "poses  = test_df[\"pos\"]\n",
    "\n",
    "known_mask_tok = tokens.isin(set(mfl_tok.keys()))\n",
    "\n",
    "#evaluate model A (no pos)\n",
    "pred_a = predict_mfl_token(tokens, fallback=\"spacy\")\n",
    "evaluate(y_true, pred_a, known_mask_tok, label=\"MFL token->lemma (fallback spaCy)\")\n",
    "\n",
    "#evaluate model B (with pos)\n",
    "known_mask_tokpos = pd.Series(list(zip(tokens, poses))).isin(set(mfl_tok_pos.keys()))\n",
    "pred_b = predict_mfl_token_pos(tokens, poses, fallback=\"spacy\")\n",
    "evaluate(y_true, pred_b, known_mask_tokpos, label=\"MFL (token,pos)->lemma (fallback spaCy)\")\n",
    "\n",
    "#spacy\n",
    "pred_spacy = [spacy_lemma(t) for t in tokens]\n",
    "evaluate(y_true, pred_spacy, None, label=\"spaCy token lemma baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Comparison with spaCy\n",
    "\n",
    "We evaluate three lemmatization approaches on the testing set:\n",
    "\n",
    "- **MFL token → lemma**: a most-frequent-lemma baseline learned from the training data, without PoS information.\n",
    "- **MFL (token, PoS) → lemma**: the same approach, but incorporating the part-of-speech tag to reduce lexical ambiguity.\n",
    "- **spaCy baseline**: lemmatization produced by the `fr_core_news_sm` model, used as a general-purpose reference.\n",
    "\n",
    "### Overall Accuracy\n",
    "\n",
    "| Model | Accuracy |\n",
    "|------|----------|\n",
    "| MFL token → lemma | 0.9531 |\n",
    "| MFL (token, PoS) → lemma | **0.9653** |\n",
    "| spaCy baseline | 0.8150 |\n",
    "\n",
    "Both proposed approaches significantly outperform the spaCy lemmatizer, with an improvement of more than **15 accuracy points**. This result is expected, as the proposed models are directly trained on the same annotation scheme and domain as the evaluation data, while spaCy is a generic lemmatizer.\n",
    "\n",
    "### Known vs Unknown Words\n",
    "\n",
    "To better understand the behavior of the models, we distinguish between words seen during training (*known words*) and unseen words (*unknown words*).\n",
    "\n",
    "| Model | Known accuracy | Unknown accuracy |\n",
    "|------|---------------|------------------|\n",
    "| MFL token → lemma | 0.9694 | 0.6988 |\n",
    "| MFL (token, PoS) → lemma | **0.9855** | 0.6828 |\n",
    "\n",
    "For known words, the accuracy is extremely high, reaching **98.6%** when PoS information is used. This shows that lemmatization is almost deterministic for observed lexical forms and that PoS tags effectively reduce ambiguity for homographic tokens (e.g., noun vs verb forms).\n",
    "\n",
    "For unknown words, performance drops to approximately **69%**, highlighting the intrinsic difficulty of generalizing to unseen forms. Interestingly, incorporating PoS information does not improve performance on unknown words and even slightly degrades it, since PoS-specific token–lemma pairs remain unseen in these cases and the prediction relies mainly on the fallback strategy.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Despite its simplicity, the most-frequent-lemma approach proves to be a very strong baseline for lemmatization. The inclusion of PoS tags provides a clear benefit for known words, while the main limitation of the approach lies in its handling of out-of-vocabulary tokens. Overall, the proposed methods demonstrate that simple supervised lexical strategies can outperform more complex general-purpose lemmatizers when the domain and annotation scheme are well matched."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_mining_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
